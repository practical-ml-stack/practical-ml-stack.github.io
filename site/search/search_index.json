{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Practical ML Stack","text":"<p>Real-World Machine Learning for Industry Problems</p> <p>Move beyond toy datasets. Build production-ready ML systems that solve real business problems.</p> <p> Get Started  Browse Use Cases</p>"},{"location":"#the-problem-were-solving","title":"The Problem We're Solving","text":"<p>You've completed the tutorials. You've trained models on IRIS, Titanic, and MNIST. You understand the algorithms.</p> <p>But when you face a real business problem, you're stuck.</p> <ul> <li>How do you handle messy, incomplete data?</li> <li>What features actually matter for churn prediction?</li> <li>How do you deploy a model that serves millions of requests?</li> <li>What does \"production-ready\" even mean?</li> </ul> <p>The gap between tutorial ML and industry ML is vast. This resource bridges that gap.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":""},{"location":"#churn-modelling","title":"Churn Modelling","text":"<p>Predict which customers will leave before they do. Learn feature engineering for behavioral data, handling class imbalance, and building interpretable models for business stakeholders.</p> <p> Explore</p>"},{"location":"#demand-forecasting","title":"Demand Forecasting","text":"<p>Build time-series models that predict inventory needs. Handle seasonality, promotions, and external factors that drive demand patterns.</p> <p> Coming Soon</p>"},{"location":"#cross-sell-modelling","title":"Cross-Sell Modelling","text":"<p>Identify the right product for the right customer at the right time. Learn recommendation techniques that drive revenue without annoying users.</p> <p> Coming Soon</p>"},{"location":"#assortment-optimization","title":"Assortment Optimization","text":"<p>Decide which products to stock in which locations. Combine ML with optimization to maximize profit under real constraints.</p> <p> Coming Soon</p>"},{"location":"#how-this-resource-is-different","title":"How This Resource Is Different","text":"Traditional Tutorials Practical ML Stack Clean, preprocessed datasets Raw, messy real-world data Focus on model accuracy Focus on business impact Single notebook experiments End-to-end pipelines Academic examples Industry case studies \"Here's the code\" \"Here's why this works\""},{"location":"#who-this-is-for","title":"Who This Is For","text":"<p>This is for you if...</p> <ul> <li>You know Python and basic ML concepts (scikit-learn, pandas)</li> <li>You've completed introductory ML courses or tutorials</li> <li>You want to solve real problems, not just pass exams</li> <li>You're preparing for ML engineering or data science roles</li> <li>You're a practitioner looking to level up your skills</li> </ul> <p>This might not be for you if...</p> <ul> <li>You're brand new to Python or programming</li> <li>You're looking for theoretical deep-dives into algorithms</li> <li>You want research-focused content (try mlsysbook.ai instead)</li> </ul>"},{"location":"#try-it-yourself","title":"Try It Yourself","text":"<p>Every use case includes:</p> <p> </p> <ul> <li>Inline code snippets with explanations</li> <li>Google Colab notebooks - run in your browser, no setup required</li> <li>Downloadable notebooks - experiment on your own machine</li> <li>Links to real datasets from Kaggle, UCI, OpenML, and more</li> </ul>"},{"location":"#built-by-practitioners-for-practitioners","title":"Built by Practitioners, For Practitioners","text":"<p>This is a community-driven project. The use cases come from real practitioners solving real problems in their day jobs.</p> <p>Want to contribute? Share your expertise and help others learn.</p> <p> Meet the Contributors  Contribute on GitHub</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to build real ML systems?</p> <p> Get Started Now</p>"},{"location":"#setup-your-environment","title":"Setup Your Environment","text":"<p>Get your local development environment ready with Python, Jupyter, and the essential libraries.</p> <p> Environment Setup</p>"},{"location":"#start-with-churn-modelling","title":"Start with Churn Modelling","text":"<p>Our most complete use case. Learn end-to-end ML workflow with a classic business problem.</p> <p> Churn Modelling</p>"},{"location":"#explore-datasets","title":"Explore Datasets","text":"<p>Curated collection of real-world datasets for each use case from trusted sources.</p> <p> Dataset Resources</p>"},{"location":"contributors/","title":"Contributors","text":"<p>Practical ML Stack is built by practitioners, for practitioners. The use cases and insights come from real people solving real problems in their day jobs.</p>"},{"location":"contributors/#our-contributors","title":"Our Contributors","text":"<p>Your Name Here</p> <p>Data Scientist</p> <p>Be the first to contribute!</p> <p></p> <p>Become a Contributor</p> <p>We're actively looking for practitioners to share their expertise. Your contribution helps thousands of people learn practical ML skills.</p>"},{"location":"contributors/#how-to-contribute","title":"How to Contribute","text":"<p>We welcome contributions from ML practitioners at all levels. Here's how you can help:</p>"},{"location":"contributors/#contribute-a-use-case","title":"Contribute a Use Case","text":"<p>Share your expertise by writing about a real-world ML problem you've solved.</p> <p>What we're looking for:</p> <ul> <li>Industry problems: Churn, forecasting, fraud, recommendations, etc.</li> <li>Real techniques: Feature engineering, model selection, deployment strategies</li> <li>Practical insights: Lessons learned, pitfalls to avoid, business context</li> <li>Reproducible code: Working examples that readers can run</li> </ul> <p>Use cases we need:</p> Use Case Industry Status Fraud Detection Finance, E-commerce  Needed Customer Segmentation Marketing, Retail  Needed Predictive Maintenance Manufacturing, IoT  Needed Credit Scoring Banking, Lending  Needed Price Optimization E-commerce, Travel  Needed Demand Forecasting Retail, Supply Chain  Needed"},{"location":"contributors/#fix-issues-or-improve-content","title":"Fix Issues or Improve Content","text":"<ul> <li>Fix typos, improve explanations, or update outdated code</li> <li>Add missing details or clarify confusing sections</li> <li>Improve code quality or add better examples</li> </ul>"},{"location":"contributors/#translate-content","title":"Translate Content","text":"<p>Help make Practical ML Stack accessible to non-English speakers.</p>"},{"location":"contributors/#contribution-process","title":"Contribution Process","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Create Branch]\n    B --&gt; C[Write Content]\n    C --&gt; D[Submit PR]\n    D --&gt; E[Review]\n    E --&gt; F[Merge!]</code></pre>"},{"location":"contributors/#step-1-fork-the-repository","title":"Step 1: Fork the Repository","text":"<pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/practical-ml-stack.github.io.git\ncd practical-ml-stack.github.io\n</code></pre>"},{"location":"contributors/#step-2-create-a-branch","title":"Step 2: Create a Branch","text":"<pre><code>git checkout -b add-fraud-detection-usecase\n</code></pre>"},{"location":"contributors/#step-3-set-up-local-environment","title":"Step 3: Set Up Local Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Start local server\nmkdocs serve\n</code></pre> <p>Visit <code>http://localhost:8000</code> to preview your changes.</p>"},{"location":"contributors/#step-4-write-your-content","title":"Step 4: Write Your Content","text":"<p>For a new use case, create files following this structure:</p> <pre><code>docs/use-cases/your-usecase/\n\u251c\u2500\u2500 index.md        # Problem overview\n\u251c\u2500\u2500 data.md         # Data understanding\n\u251c\u2500\u2500 features.md     # Feature engineering\n\u251c\u2500\u2500 modelling.md    # Model building\n\u2514\u2500\u2500 deployment.md   # Deployment considerations\n\nnotebooks/\n\u2514\u2500\u2500 your-usecase.ipynb  # Runnable notebook\n</code></pre> <p>Use our Use Case Template below.</p>"},{"location":"contributors/#step-5-create-your-contributor-profile","title":"Step 5: Create Your Contributor Profile","text":"<p>Create a file at <code>docs/contributors/profiles/your-name.md</code>:</p> <pre><code>---\ntitle: Your Name\n---\n\n# Your Name\n\n![Your Photo](your-photo-url.jpg)\n\n**Role:** Data Scientist at Company Name\n\n**Expertise:** Customer Analytics, Time Series, MLOps\n\n**Contributions:**\n- [Fraud Detection](../../use-cases/fraud-detection/index.md)\n\n## About\n\nBrief bio about yourself and your experience with ML.\n\n## Connect\n\n- [:fontawesome-brands-linkedin: LinkedIn](https://linkedin.com/in/yourprofile)\n- [:fontawesome-brands-github: GitHub](https://github.com/yourusername)\n- [:fontawesome-brands-twitter: Twitter](https://twitter.com/yourhandle)\n</code></pre>"},{"location":"contributors/#step-6-submit-a-pull-request","title":"Step 6: Submit a Pull Request","text":"<pre><code>git add .\ngit commit -m \"Add fraud detection use case\"\ngit push origin add-fraud-detection-usecase\n</code></pre> <p>Then open a Pull Request on GitHub.</p>"},{"location":"contributors/#use-case-template","title":"Use Case Template","text":"<p>Use this template when creating a new use case:</p>"},{"location":"contributors/#indexmd-problem-overview","title":"index.md (Problem Overview)","text":"<pre><code>---\ntitle: Use Case Name\ndescription: Brief description for SEO\n---\n\n# Use Case Name\n\n[![Open In Colab](colab-badge-url)](colab-link)\n\n**One-line problem statement.**\n\n## What is [Problem]?\n\nExplain the business problem.\n\n## Why It Matters\n\nBusiness impact and value.\n\n## Approach Overview\n\nHigh-level approach diagram.\n\n## Prerequisites\n\nWhat readers need to know.\n\n## Datasets\n\nWhere to get data.\n</code></pre>"},{"location":"contributors/#content-guidelines","title":"Content Guidelines","text":"<ul> <li>Write for practitioners: Assume Python/ML basics, explain domain specifics</li> <li>Focus on \"why\": Don't just show code\u2014explain the reasoning</li> <li>Use real numbers: Include actual metrics, business impact</li> <li>Be honest about tradeoffs: Every decision has pros and cons</li> <li>Test your code: Everything should run without errors</li> </ul>"},{"location":"contributors/#recognition","title":"Recognition","text":"<p>Contributors are recognized in multiple ways:</p> <ol> <li>Contributor page: Your profile listed on this page</li> <li>Author attribution: Your name on every use case you contribute</li> <li>Social sharing: We promote contributions on our channels</li> <li>Community: Join our contributor community</li> </ol>"},{"location":"contributors/#code-of-conduct","title":"Code of Conduct","text":"<p>We're committed to providing a welcoming and inclusive environment. All contributors are expected to:</p> <ul> <li>Be respectful and constructive in feedback</li> <li>Welcome newcomers and help them contribute</li> <li>Focus on what's best for the community</li> <li>Show empathy towards other community members</li> </ul>"},{"location":"contributors/#questions","title":"Questions?","text":"<ul> <li>GitHub Discussions: Ask questions</li> <li>Issues: Report problems</li> </ul>"},{"location":"contributors/#ready-to-contribute","title":"Ready to Contribute?","text":""},{"location":"contributors/#fork-the-repo","title":"Fork the Repo","text":"<p>Start by forking the repository on GitHub.</p> <p> Fork on GitHub</p>"},{"location":"contributors/#use-case-template_1","title":"Use Case Template","text":"<p>Download our template to get started quickly.</p> <p> View Template</p>"},{"location":"contributors/profiles/template/","title":"[Your Name]","text":""},{"location":"contributors/profiles/template/#about","title":"About","text":"<p>Current Role: [Your Title] at [Company Name]</p> <p>Location: [City, Country]</p> <p>Experience: [X] years in [field]</p>"},{"location":"contributors/profiles/template/#expertise","title":"Expertise","text":"<ul> <li>Area of expertise 1</li> <li>Area of expertise 2</li> <li>Area of expertise 3</li> </ul>"},{"location":"contributors/profiles/template/#contributions-to-practical-ml-stack","title":"Contributions to Practical ML Stack","text":"Use Case Description Date Use Case Name Brief description Month Year"},{"location":"contributors/profiles/template/#bio","title":"Bio","text":"<p>Write a brief bio about yourself (2-3 paragraphs). Include:</p> <ul> <li>Your background and how you got into ML/Data Science</li> <li>What industries or problems you've worked on</li> <li>What you're passionate about in the ML space</li> </ul>"},{"location":"contributors/profiles/template/#connect","title":"Connect","text":"<p> LinkedIn  GitHub  Twitter  Website</p>"},{"location":"contributors/profiles/template/#instructions-for-contributors","title":"Instructions for Contributors","text":"<p>How to use this template</p> <ol> <li>Copy this file to <code>docs/contributors/profiles/your-name.md</code></li> <li>Replace all placeholder content with your information</li> <li>Delete this instruction box</li> <li>Submit a PR with your profile</li> </ol> <p>Required fields:</p> <ul> <li>Name</li> <li>Current role</li> <li>At least one contribution</li> <li>Brief bio</li> <li>At least one social link</li> </ul> <p>Optional fields:</p> <ul> <li>Photo (recommended)</li> <li>Location</li> <li>Full expertise list</li> <li>Website</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to Practical ML Stack! This guide will help you understand what you need to know before diving in, and how to get the most value from this resource.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before starting with the use cases, you should be comfortable with:</p>"},{"location":"getting-started/#python-fundamentals","title":"Python Fundamentals","text":"<ul> <li> Variables, data types, and basic operations</li> <li> Functions and classes</li> <li> List comprehensions and generators</li> <li> File I/O and working with JSON/CSV</li> <li> Virtual environments and pip</li> </ul> <p>Not there yet?</p> <p>We recommend Python for Everybody or the Official Python Tutorial.</p>"},{"location":"getting-started/#data-manipulation","title":"Data Manipulation","text":"<ul> <li> pandas: DataFrames, Series, groupby, merge, pivot</li> <li> NumPy: Arrays, broadcasting, basic linear algebra</li> <li> Data cleaning: handling missing values, duplicates, outliers</li> </ul> <p>Not there yet?</p> <p>Try Kaggle's Pandas Course (free, ~4 hours).</p>"},{"location":"getting-started/#machine-learning-basics","title":"Machine Learning Basics","text":"<ul> <li> Supervised vs unsupervised learning</li> <li> Train/test splits and cross-validation</li> <li> Common algorithms: Linear Regression, Logistic Regression, Decision Trees, Random Forest</li> <li> Evaluation metrics: accuracy, precision, recall, F1, AUC-ROC</li> <li> Basic scikit-learn workflow: fit, predict, score</li> </ul> <p>Not there yet?</p> <p>Scikit-learn's Getting Started and Kaggle's Intro to ML are great starting points.</p>"},{"location":"getting-started/#what-you-dont-need","title":"What You Don't Need","text":"<p>Don't worry if you haven't mastered these yet\u2014you'll learn them along the way:</p> <ul> <li> Deep learning frameworks (TensorFlow, PyTorch)</li> <li> MLOps tools (MLflow, Kubeflow, Airflow)</li> <li> Cloud platforms (AWS, GCP, Azure)</li> <li> Advanced statistics or calculus</li> <li> Software engineering best practices</li> </ul> <p>We'll introduce these concepts as needed in each use case.</p>"},{"location":"getting-started/#how-to-use-this-resource","title":"How to Use This Resource","text":""},{"location":"getting-started/#choose-a-use-case","title":"Choose a Use Case","text":"<p>Start with a problem that interests you or relates to your work:</p> Use Case Best For Difficulty Churn Modelling Classification, business metrics  Beginner-friendly Demand Forecasting Time series, seasonality  Intermediate Cross-Sell Modelling Recommendations, ranking  Intermediate Assortment Optimization Optimization, constraints  Advanced <p>First Time Here?</p> <p>Start with Churn Modelling. It's our most complete use case and covers the end-to-end ML workflow.</p>"},{"location":"getting-started/#read-the-concepts","title":"Read the Concepts","text":"<p>Each use case has multiple pages covering:</p> <ol> <li>Problem Overview - Business context, why it matters</li> <li>Data Understanding - What data you need, where to get it</li> <li>Feature Engineering - Creating meaningful features from raw data</li> <li>Model Building - Training, tuning, and evaluating models</li> <li>Deployment - Taking your model to production</li> </ol> <p>Read through the concepts and code explanations. Understand the why behind each decision.</p>"},{"location":"getting-started/#run-the-code","title":"Run the Code","text":"<p>Every use case includes runnable notebooks:</p> <p> </p> <p>Option A: Google Colab (recommended for beginners)</p> <ul> <li>Click \"Open in Colab\" badge</li> <li>No setup required\u2014runs in your browser</li> <li>Free GPU access for larger models</li> </ul> <p>Option B: Local Environment</p> <ul> <li>Download the notebook</li> <li>Follow our Environment Setup guide</li> <li>Run with your own modifications</li> </ul>"},{"location":"getting-started/#experiment","title":"Experiment","text":"<p>The real learning happens when you:</p> <ul> <li>Modify the code and see what changes</li> <li>Try different features or algorithms</li> <li>Apply the techniques to your own data</li> <li>Break things and fix them</li> </ul>"},{"location":"getting-started/#learning-path","title":"Learning Path","text":""},{"location":"getting-started/#beginner-path-4-6-weeks","title":"Beginner Path (4-6 weeks)","text":"<pre><code>graph LR\n    A[Prerequisites Check] --&gt; B[Environment Setup]\n    B --&gt; C[Churn Modelling]\n    C --&gt; D[Try Your Own Data]\n    D --&gt; E[Demand Forecasting]</code></pre> <ol> <li>Week 1: Review prerequisites, set up environment</li> <li>Week 2-3: Complete Churn Modelling use case</li> <li>Week 4: Apply to your own dataset</li> <li>Week 5-6: Start Demand Forecasting</li> </ol>"},{"location":"getting-started/#practitioner-path-2-3-weeks","title":"Practitioner Path (2-3 weeks)","text":"<p>Already comfortable with ML basics? Jump straight in:</p> <ol> <li>Day 1: Skim prerequisites, set up environment</li> <li>Week 1: Deep dive into most relevant use case</li> <li>Week 2: Apply to your work problem</li> <li>Week 3: Explore advanced topics (deployment, monitoring)</li> </ol>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":""},{"location":"getting-started/#github-issues","title":"GitHub Issues","text":"<p>Found a bug or have a suggestion? Open an issue.</p>"},{"location":"getting-started/#discussions","title":"Discussions","text":"<p>Have questions or want to share your work? Join the GitHub Discussions.</p>"},{"location":"getting-started/#community","title":"Community","text":"<p>Connect with other practitioners:</p> <ul> <li>Share your implementations</li> <li>Get feedback on your approach</li> <li>Learn from others' experiences</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>Get Python, Jupyter, and essential libraries installed.</p> <p> Environment Setup</p>"},{"location":"getting-started/#start-learning","title":"Start Learning","text":"<p>Jump into your first use case with Churn Modelling.</p> <p> Churn Modelling</p>"},{"location":"getting-started/environment/","title":"Environment Setup","text":"<p>This guide walks you through setting up a local development environment for running the notebooks and experimenting with your own data.</p> <p>Just Want to Try It?</p> <p>You can skip local setup entirely and use Google Colab. Every use case has an \"Open in Colab\" button that runs in your browser with no installation required.</p>"},{"location":"getting-started/environment/#quick-setup-5-minutes","title":"Quick Setup (5 minutes)","text":"<p>For those who know what they're doing:</p> pipcondauv (fast) <pre><code># Create virtual environment\npython -m venv practical-ml-env\nsource practical-ml-env/bin/activate  # On Windows: practical-ml-env\\Scripts\\activate\n\n# Install dependencies\npip install pandas numpy scikit-learn matplotlib seaborn jupyter\npip install xgboost lightgbm catboost  # Optional: gradient boosting\n\n# Launch Jupyter\njupyter notebook\n</code></pre> <pre><code># Create conda environment\nconda create -n practical-ml python=3.10\nconda activate practical-ml\n\n# Install dependencies\nconda install pandas numpy scikit-learn matplotlib seaborn jupyter\nconda install -c conda-forge xgboost lightgbm catboost  # Optional\n\n# Launch Jupyter\njupyter notebook\n</code></pre> <pre><code># Install uv if you haven't\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create environment and install\nuv venv practical-ml-env\nsource practical-ml-env/bin/activate\nuv pip install pandas numpy scikit-learn matplotlib seaborn jupyter\n\n# Launch Jupyter\njupyter notebook\n</code></pre>"},{"location":"getting-started/environment/#detailed-setup-guide","title":"Detailed Setup Guide","text":""},{"location":"getting-started/environment/#step-1-install-python","title":"Step 1: Install Python","text":"<p>We recommend Python 3.10 or 3.11 for best compatibility.</p> macOSWindowsLinux <pre><code># Using Homebrew (recommended)\nbrew install python@3.11\n\n# Verify installation\npython3 --version\n</code></pre> <p>Or download from python.org.</p> <ol> <li>Download Python from python.org</li> <li>Run the installer</li> <li>Important: Check \"Add Python to PATH\"</li> <li>Verify in Command Prompt:</li> </ol> <pre><code>python --version\n</code></pre> <pre><code># Ubuntu/Debian\nsudo apt update\nsudo apt install python3.11 python3.11-venv python3-pip\n\n# Verify installation\npython3 --version\n</code></pre>"},{"location":"getting-started/environment/#step-2-create-a-virtual-environment","title":"Step 2: Create a Virtual Environment","text":"<p>Why Virtual Environments?</p> <p>Virtual environments keep your project dependencies isolated. Different projects can use different package versions without conflicts.</p> <pre><code># Navigate to your projects folder\ncd ~/projects  # or wherever you keep your code\n\n# Create a new directory for this work\nmkdir practical-ml-stack\ncd practical-ml-stack\n\n# Create virtual environment\npython3 -m venv venv\n\n# Activate it\nsource venv/bin/activate  # macOS/Linux\n# OR\nvenv\\Scripts\\activate     # Windows\n</code></pre> <p>You should see <code>(venv)</code> at the beginning of your terminal prompt.</p>"},{"location":"getting-started/environment/#step-3-install-core-dependencies","title":"Step 3: Install Core Dependencies","text":"<p>Create a <code>requirements.txt</code> file with the following contents:</p> requirements.txt<pre><code># Core data science\npandas&gt;=2.0.0\nnumpy&gt;=1.24.0\nscikit-learn&gt;=1.3.0\n\n# Visualization\nmatplotlib&gt;=3.7.0\nseaborn&gt;=0.12.0\nplotly&gt;=5.15.0\n\n# Jupyter\njupyter&gt;=1.0.0\njupyterlab&gt;=4.0.0\nnotebook&gt;=7.0.0\n\n# Gradient Boosting (optional but recommended)\nxgboost&gt;=2.0.0\nlightgbm&gt;=4.0.0\ncatboost&gt;=1.2.0\n\n# Utilities\ntqdm&gt;=4.65.0\npython-dotenv&gt;=1.0.0\n</code></pre> <p>Install all dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/environment/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<p>Run this quick test to make sure everything works:</p> <pre><code># test_setup.py\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\n\nprint(f\"pandas: {pd.__version__}\")\nprint(f\"numpy: {np.__version__}\")\nprint(f\"scikit-learn: {sklearn.__version__}\")\n\n# Quick test\ndf = pd.DataFrame({'x': [1, 2, 3], 'y': [2, 4, 6]})\nprint(f\"\\nTest DataFrame:\\n{df}\")\nprint(\"\\n\u2705 All good! Your environment is ready.\")\n</code></pre> <pre><code>python test_setup.py\n</code></pre>"},{"location":"getting-started/environment/#step-5-launch-jupyter","title":"Step 5: Launch Jupyter","text":"<pre><code># Classic Notebook interface\njupyter notebook\n\n# OR JupyterLab (more features)\njupyter lab\n</code></pre> <p>Your browser should open automatically. If not, look for a URL in the terminal output like: <pre><code>http://localhost:8888/?token=abc123...\n</code></pre></p>"},{"location":"getting-started/environment/#ide-setup-optional","title":"IDE Setup (Optional)","text":""},{"location":"getting-started/environment/#vs-code","title":"VS Code","text":"<p>VS Code with the Python extension is excellent for ML work:</p> <ol> <li>Install VS Code</li> <li>Install the Python extension</li> <li>Install the Jupyter extension</li> <li>Open your project folder</li> <li>Select your virtual environment (bottom-left status bar)</li> </ol>"},{"location":"getting-started/environment/#pycharm","title":"PyCharm","text":"<ol> <li>Install PyCharm (Community edition is free)</li> <li>Open your project folder</li> <li>Configure interpreter: Settings \u2192 Project \u2192 Python Interpreter</li> <li>Select your virtual environment</li> </ol>"},{"location":"getting-started/environment/#google-colab-alternative","title":"Google Colab Alternative","text":"<p>If you prefer not to set up a local environment:</p>"},{"location":"getting-started/environment/#pros-of-colab","title":"Pros of Colab","text":"<ul> <li> No installation required</li> <li> Free GPU/TPU access</li> <li> Pre-installed ML libraries</li> <li> Easy sharing and collaboration</li> </ul>"},{"location":"getting-started/environment/#cons-of-colab","title":"Cons of Colab","text":"<ul> <li> Requires internet connection</li> <li> Sessions timeout after inactivity</li> <li> Limited customization</li> <li> Slower for small operations (network overhead)</li> </ul>"},{"location":"getting-started/environment/#using-colab-with-our-notebooks","title":"Using Colab with Our Notebooks","text":"<ol> <li>Click the \"Open in Colab\" badge on any use case page</li> <li>The notebook will open in Colab</li> <li>Click \"Copy to Drive\" to save your own version</li> <li>Run cells with Shift+Enter</li> </ol>"},{"location":"getting-started/environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/environment/#common-issues","title":"Common Issues","text":"pip install fails with permission error <p>Make sure you've activated your virtual environment: <pre><code>source venv/bin/activate  # macOS/Linux\nvenv\\Scripts\\activate     # Windows\n</code></pre></p> <p>If still failing, try: <pre><code>pip install --user package-name\n</code></pre></p> Jupyter can't find my virtual environment <p>Install the IPython kernel in your environment: <pre><code>pip install ipykernel\npython -m ipykernel install --user --name=practical-ml\n</code></pre></p> <p>Then select \"practical-ml\" as the kernel in Jupyter.</p> Import errors after installation <p>Make sure you're running Jupyter from within your virtual environment: <pre><code># Activate environment first\nsource venv/bin/activate\n\n# Then launch Jupyter\njupyter notebook\n</code></pre></p> matplotlib plots not showing <p>In Jupyter, add this at the top of your notebook: <pre><code>%matplotlib inline\n</code></pre></p>"},{"location":"getting-started/environment/#package-versions-reference","title":"Package Versions Reference","text":"<p>These are the versions we've tested with:</p> Package Version Notes Python 3.10, 3.11 3.12 may have compatibility issues pandas 2.0+ Major API changes from 1.x scikit-learn 1.3+ XGBoost 2.0+ New API in 2.0 LightGBM 4.0+"},{"location":"getting-started/environment/#next-steps","title":"Next Steps","text":"<p>Your environment is ready! Time to start learning:</p>"},{"location":"getting-started/environment/#churn-modelling","title":"Churn Modelling","text":"<p>Start with our most complete use case.</p> <p> Start Learning</p>"},{"location":"getting-started/environment/#explore-datasets","title":"Explore Datasets","text":"<p>Find real-world datasets to practice with.</p> <p> Dataset Resources</p>"},{"location":"resources/datasets/","title":"Datasets","text":"<p>Finding the right dataset is often the first challenge in any ML project. This page curates high-quality, real-world datasets organized by use case.</p>"},{"location":"resources/datasets/#dataset-sources","title":"Dataset Sources","text":""},{"location":"resources/datasets/#primary-sources","title":"Primary Sources","text":""},{"location":"resources/datasets/#kaggle-datasets","title":"Kaggle Datasets","text":"<p>The largest repository of ML datasets. Great for:</p> <ul> <li>Competition datasets with benchmarks</li> <li>Community-uploaded real-world data</li> <li>Notebooks showing how others approached problems</li> </ul>"},{"location":"resources/datasets/#uci-ml-repository","title":"UCI ML Repository","text":"<p>Classic ML datasets used in academic research:</p> <ul> <li>Well-documented and clean</li> <li>Widely cited in literature</li> <li>Good for benchmarking</li> </ul>"},{"location":"resources/datasets/#google-dataset-search","title":"Google Dataset Search","text":"<p>Search engine for datasets across the web:</p> <ul> <li>Aggregates from multiple sources</li> <li>Includes government and research data</li> <li>Good for finding niche datasets</li> </ul>"},{"location":"resources/datasets/#openml","title":"OpenML","text":"<p>Platform for sharing ML experiments:</p> <ul> <li>Standardized dataset format</li> <li>Includes benchmark results</li> <li>API for programmatic access</li> </ul>"},{"location":"resources/datasets/#other-sources","title":"Other Sources","text":"Source Best For Notes Hugging Face Datasets NLP, Computer Vision Growing collection, easy to load AWS Open Data Large-scale data Free to access, cloud-native Data.gov Government data US federal data, various domains Zenodo Research data Academic datasets with DOIs Figshare Research data Includes supplementary materials DataHub.io Curated collections Clean, well-documented"},{"location":"resources/datasets/#datasets-by-use-case","title":"Datasets by Use Case","text":""},{"location":"resources/datasets/#customer-churn","title":"Customer Churn","text":"Dataset Size Source Description Telco Customer Churn 7K Kaggle Telecom churn with demographics and services Bank Customer Churn 10K Kaggle Banking churn with geography and products KKBox Churn 2.6M Kaggle Music streaming service E-Commerce Churn 5K Kaggle Online retail churn"},{"location":"resources/datasets/#demand-forecasting","title":"Demand Forecasting","text":"Dataset Size Source Description Store Sales - Kaggle 3M Kaggle Ecuadorian grocery store sales M5 Forecasting 46M Kaggle Walmart sales, hierarchical Rossmann Store Sales 1M Kaggle German drugstore chain Web Traffic Forecasting 145K Kaggle Wikipedia page views"},{"location":"resources/datasets/#fraud-detection","title":"Fraud Detection","text":"Dataset Size Source Description Credit Card Fraud 284K Kaggle Anonymized transactions, highly imbalanced IEEE-CIS Fraud 590K Kaggle E-commerce transactions Synthetic Fraud 6.3M Kaggle Simulated mobile money transactions"},{"location":"resources/datasets/#recommendations-cross-sell","title":"Recommendations / Cross-Sell","text":"Dataset Size Source Description Amazon Product Reviews 568K Kaggle Food reviews with ratings MovieLens 25M GroupLens Movie ratings, multiple sizes Instacart Orders 3.4M Kaggle Grocery orders with products Retail Transactions 541K Kaggle UK online retail transactions"},{"location":"resources/datasets/#pricing-revenue","title":"Pricing / Revenue","text":"Dataset Size Source Description Mercari Price Suggestion 1.4M Kaggle Product pricing Airbnb Listings Varies Inside Airbnb Rental prices by city Used Cars 426K Kaggle Vehicle pricing"},{"location":"resources/datasets/#customer-analytics","title":"Customer Analytics","text":"Dataset Size Source Description Marketing Campaign 2K Kaggle Customer response to campaigns Customer Segmentation 200 Kaggle RFM-style data Brazilian E-Commerce 100K Kaggle Orders, reviews, sellers"},{"location":"resources/datasets/#how-to-choose-a-dataset","title":"How to Choose a Dataset","text":""},{"location":"resources/datasets/#for-learning","title":"For Learning","text":"<ol> <li>Start small (&lt; 100K rows) - faster iteration</li> <li>Well-documented - clear feature descriptions</li> <li>Known benchmarks - compare your results</li> <li>Clean enough - some data quality issues, but not overwhelming</li> </ol>"},{"location":"resources/datasets/#for-portfolio-projects","title":"For Portfolio Projects","text":"<ol> <li>Interesting domain - something you can talk about in interviews</li> <li>Real-world messiness - shows you can handle imperfect data</li> <li>Business relevance - demonstrates you understand business problems</li> <li>Unique angle - don't just reproduce existing notebooks</li> </ol>"},{"location":"resources/datasets/#for-production-practice","title":"For Production Practice","text":"<ol> <li>Large scale (&gt; 1M rows) - practice with real volumes</li> <li>Multiple tables - practice joins and feature engineering</li> <li>Time component - practice proper train/test splits</li> <li>Ongoing updates - practice with data pipelines</li> </ol>"},{"location":"resources/datasets/#loading-datasets","title":"Loading Datasets","text":""},{"location":"resources/datasets/#kaggle-api","title":"Kaggle API","text":"<pre><code># Install and configure\n# pip install kaggle\n# Place kaggle.json in ~/.kaggle/\n\nimport kaggle\n\n# Download dataset\nkaggle.api.dataset_download_files(\n    'blastchar/telco-customer-churn',\n    path='./data',\n    unzip=True\n)\n</code></pre>"},{"location":"resources/datasets/#openml_1","title":"OpenML","text":"<pre><code>from sklearn.datasets import fetch_openml\n\n# Load dataset by name or ID\ndata = fetch_openml(name='credit-g', version=1, as_frame=True)\ndf = data.frame\n</code></pre>"},{"location":"resources/datasets/#hugging-face","title":"Hugging Face","text":"<pre><code>from datasets import load_dataset\n\n# Load dataset\ndataset = load_dataset(\"imdb\")\n</code></pre>"},{"location":"resources/datasets/#direct-download","title":"Direct Download","text":"<pre><code>import pandas as pd\n\n# Many Kaggle datasets have direct URLs\nurl = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\ndf = pd.read_csv(url)\n</code></pre>"},{"location":"resources/datasets/#data-licensing","title":"Data Licensing","text":"<p>Check the License</p> <p>Always check dataset licenses before using data, especially for:</p> <ul> <li>Commercial projects</li> <li>Publishing results</li> <li>Redistributing data</li> </ul> <p>Common licenses:</p> License Commercial Use Attribution Required CC0 \u2705 Yes \u274c No CC-BY \u2705 Yes \u2705 Yes CC-BY-NC \u274c No \u2705 Yes Research Only \u274c No \u2705 Yes"},{"location":"resources/datasets/#request-a-dataset","title":"Request a Dataset","text":"<p>Can't find a dataset for your use case? Open an issue and we'll try to help!</p>"},{"location":"resources/tools/","title":"Tools &amp; Libraries","text":"<p>A curated list of tools and libraries used in real ML workflows. Organized by stage of the ML lifecycle.</p>"},{"location":"resources/tools/#core-data-science-stack","title":"Core Data Science Stack","text":""},{"location":"resources/tools/#essential-libraries","title":"Essential Libraries","text":"Library Purpose Install pandas Data manipulation <code>pip install pandas</code> NumPy Numerical computing <code>pip install numpy</code> scikit-learn ML algorithms <code>pip install scikit-learn</code> Matplotlib Visualization <code>pip install matplotlib</code> Seaborn Statistical visualization <code>pip install seaborn</code>"},{"location":"resources/tools/#gradient-boosting","title":"Gradient Boosting","text":"Library Strengths Install XGBoost Speed, accuracy, widely used <code>pip install xgboost</code> LightGBM Very fast, handles large data <code>pip install lightgbm</code> CatBoost Best for categorical features <code>pip install catboost</code> <p>Which to choose?</p> <ul> <li>XGBoost: Best default choice, most documentation</li> <li>LightGBM: When speed matters or data is large</li> <li>CatBoost: When you have many categorical features</li> </ul>"},{"location":"resources/tools/#experiment-tracking","title":"Experiment Tracking","text":"<p>Track experiments, compare runs, and reproduce results.</p> Tool Type Best For MLflow Open source Local/team use, model registry Weights &amp; Biases Cloud/self-hosted Visualization, collaboration Neptune.ai Cloud Team collaboration DVC Open source Data versioning + experiments"},{"location":"resources/tools/#mlflow-quick-start","title":"MLflow Quick Start","text":"<pre><code>import mlflow\n\n# Start experiment\nmlflow.set_experiment(\"churn-prediction\")\n\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_param(\"model\", \"xgboost\")\n    mlflow.log_param(\"max_depth\", 5)\n\n    # Train model...\n\n    # Log metrics\n    mlflow.log_metric(\"auc\", 0.85)\n    mlflow.log_metric(\"precision\", 0.72)\n\n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n</code></pre>"},{"location":"resources/tools/#feature-engineering","title":"Feature Engineering","text":""},{"location":"resources/tools/#feature-stores","title":"Feature Stores","text":"Tool Type Best For Feast Open source Getting started, flexible Tecton Managed Enterprise, real-time Hopsworks Open source + managed Full platform"},{"location":"resources/tools/#feature-libraries","title":"Feature Libraries","text":"Library Purpose Install Feature-engine Sklearn-compatible transformers <code>pip install feature-engine</code> category_encoders Categorical encoding <code>pip install category_encoders</code> tsfresh Time series features <code>pip install tsfresh</code>"},{"location":"resources/tools/#model-deployment","title":"Model Deployment","text":""},{"location":"resources/tools/#serving-frameworks","title":"Serving Frameworks","text":"Tool Type Best For FastAPI Web framework Simple REST APIs BentoML ML serving Standardized serving Seldon Kubernetes-native Scale, enterprise TensorFlow Serving TF models High performance"},{"location":"resources/tools/#fastapi-example","title":"FastAPI Example","text":"<pre><code>from fastapi import FastAPI\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load(\"model.pkl\")\n\n@app.post(\"/predict\")\ndef predict(features: dict):\n    prediction = model.predict([list(features.values())])\n    return {\"prediction\": prediction[0]}\n</code></pre>"},{"location":"resources/tools/#data-validation","title":"Data Validation","text":"Tool Purpose Install Great Expectations Data quality testing <code>pip install great_expectations</code> Pandera DataFrame validation <code>pip install pandera</code> pydantic Data validation <code>pip install pydantic</code>"},{"location":"resources/tools/#pandera-example","title":"Pandera Example","text":"<pre><code>import pandera as pa\n\nschema = pa.DataFrameSchema({\n    \"tenure\": pa.Column(int, pa.Check.ge(0)),\n    \"monthly_charges\": pa.Column(float, pa.Check.between(0, 500)),\n    \"churn\": pa.Column(int, pa.Check.isin([0, 1]))\n})\n\n# Validate DataFrame\nschema.validate(df)\n</code></pre>"},{"location":"resources/tools/#workflow-orchestration","title":"Workflow Orchestration","text":"Tool Type Best For Apache Airflow Open source General workflows Prefect Open source + cloud Modern, Pythonic Dagster Open source Data-aware orchestration Kubeflow Pipelines Kubernetes ML-specific pipelines"},{"location":"resources/tools/#model-monitoring","title":"Model Monitoring","text":"Tool Type Best For Evidently Open source Data/model drift Whylogs Open source Data profiling NannyML Open source Performance monitoring Arize Managed Full observability"},{"location":"resources/tools/#evidently-example","title":"Evidently Example","text":"<pre><code>from evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset\n\nreport = Report(metrics=[DataDriftPreset()])\nreport.run(reference_data=train_df, current_data=production_df)\nreport.save_html(\"drift_report.html\")\n</code></pre>"},{"location":"resources/tools/#development-environment","title":"Development Environment","text":""},{"location":"resources/tools/#ides","title":"IDEs","text":"Tool Best For VS Code General development, notebooks PyCharm Pure Python projects JupyterLab Exploration, notebooks"},{"location":"resources/tools/#environment-management","title":"Environment Management","text":"Tool Purpose Install conda Environment + package management Miniconda venv Built-in virtual environments Included in Python uv Fast pip replacement <code>curl -LsSf https://astral.sh/uv/install.sh \\| sh</code> Poetry Dependency management <code>pip install poetry</code>"},{"location":"resources/tools/#recommended-stack-by-stage","title":"Recommended Stack by Stage","text":""},{"location":"resources/tools/#learning","title":"Learning","text":"<pre><code>pandas + scikit-learn + matplotlib + Jupyter\n</code></pre>"},{"location":"resources/tools/#team-projects","title":"Team Projects","text":"<pre><code>+ MLflow + DVC + Great Expectations\n</code></pre>"},{"location":"resources/tools/#production","title":"Production","text":"<pre><code>+ Airflow/Prefect + FastAPI/BentoML + Evidently\n</code></pre>"},{"location":"resources/tools/#learning-resources","title":"Learning Resources","text":""},{"location":"resources/tools/#books","title":"Books","text":"<ul> <li>Hands-On Machine Learning by Aur\u00e9lien G\u00e9ron</li> <li>Designing Machine Learning Systems by Chip Huyen</li> <li>Machine Learning Engineering by Andriy Burkov</li> </ul>"},{"location":"resources/tools/#courses","title":"Courses","text":"<ul> <li>Fast.ai - Practical deep learning</li> <li>Made With ML - MLOps</li> <li>Full Stack Deep Learning - Production ML</li> </ul>"},{"location":"resources/tools/#blogs","title":"Blogs","text":"<ul> <li>Chip Huyen's Blog</li> <li>Eugene Yan's Blog</li> <li>ML Ops Community</li> </ul>"},{"location":"use-cases/","title":"Use Cases","text":"<p>Real-world machine learning problems solved end-to-end. Each use case covers the complete journey from business problem to production deployment.</p>"},{"location":"use-cases/#available-use-cases","title":"Available Use Cases","text":""},{"location":"use-cases/#churn-modelling","title":"Churn Modelling","text":"<p>Predict which customers will leave before they do.</p> <p>Churn prediction is one of the most impactful ML applications in business. Learn to build models that identify at-risk customers, enabling proactive retention strategies.</p> Aspect Details Industry Telecom, SaaS, Banking, Retail Problem Type Binary Classification Difficulty  Beginner-friendly Time to Complete 4-6 hours <p>You'll Learn:</p> <ul> <li>Feature engineering for behavioral data</li> <li>Handling class imbalance</li> <li>Model interpretability for stakeholders</li> <li>Business metrics (CLV, retention cost)</li> </ul> <p> Start Churn Modelling</p>"},{"location":"use-cases/#demand-forecasting","title":"Demand Forecasting","text":"<p>Predict future demand to optimize inventory and resources.</p> <p>Time series forecasting for retail, supply chain, and resource planning. Handle seasonality, promotions, and external factors.</p> Aspect Details Industry Retail, Manufacturing, Logistics Problem Type Time Series Regression Difficulty  Intermediate Time to Complete 6-8 hours <p>You'll Learn:</p> <ul> <li>Time series feature engineering</li> <li>Handling seasonality and trends</li> <li>Promotional impact modeling</li> <li>Forecast accuracy metrics (MAPE, WMAPE)</li> </ul> <p> Coming Soon</p>"},{"location":"use-cases/#cross-sell-modelling","title":"Cross-Sell Modelling","text":"<p>Recommend the right product to the right customer.</p> <p>Build recommendation systems that drive revenue without annoying customers. Balance personalization with business constraints.</p> Aspect Details Industry Banking, Insurance, E-commerce Problem Type Multi-class Classification / Ranking Difficulty  Intermediate Time to Complete 6-8 hours <p>You'll Learn:</p> <ul> <li>Propensity modeling</li> <li>Next-best-action frameworks</li> <li>A/B testing recommendations</li> <li>Uplift modeling basics</li> </ul> <p> Coming Soon</p>"},{"location":"use-cases/#assortment-optimization","title":"Assortment Optimization","text":"<p>Decide which products to stock in which locations.</p> <p>Combine ML predictions with optimization to maximize profit under real-world constraints like shelf space and supplier agreements.</p> Aspect Details Industry Retail, CPG, Grocery Problem Type Optimization + ML Difficulty  Advanced Time to Complete 8-10 hours <p>You'll Learn:</p> <ul> <li>Demand prediction at SKU-store level</li> <li>Mathematical optimization basics</li> <li>Constraint handling</li> <li>What-if scenario analysis</li> </ul> <p> Coming Soon</p>"},{"location":"use-cases/#beat-planning","title":"Beat Planning","text":"<p>Optimize sales routes and territory assignments.</p> <p>Help field sales teams cover more ground efficiently. Balance workload, travel time, and customer coverage.</p> Aspect Details Industry FMCG, Pharma, Field Services Problem Type Optimization + Clustering Difficulty  Advanced Time to Complete 8-10 hours <p>You'll Learn:</p> <ul> <li>Geospatial clustering</li> <li>Vehicle routing problems (VRP)</li> <li>Workload balancing</li> <li>Territory optimization</li> </ul> <p> Coming Soon</p>"},{"location":"use-cases/#price-optimization","title":"Price Optimization","text":"<p>Set optimal prices to maximize revenue or profit.</p> <p>Dynamic pricing strategies using demand elasticity modeling. Balance short-term revenue with long-term customer relationships.</p> Aspect Details Industry E-commerce, Airlines, Hotels Problem Type Regression + Optimization Difficulty  Advanced Time to Complete 8-10 hours <p>You'll Learn:</p> <ul> <li>Price elasticity modeling</li> <li>Competitive pricing analysis</li> <li>A/B testing for pricing</li> <li>Revenue management basics</li> </ul> <p> Coming Soon</p>"},{"location":"use-cases/#use-case-structure","title":"Use Case Structure","text":"<p>Every use case follows a consistent structure:</p> <pre><code>graph LR\n    A[Problem Overview] --&gt; B[Data Understanding]\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Model Building]\n    D --&gt; E[Deployment]</code></pre>"},{"location":"use-cases/#1-problem-overview","title":"1. Problem Overview","text":"<ul> <li>Business context and why it matters</li> <li>Success metrics and KPIs</li> <li>Common pitfalls and challenges</li> </ul>"},{"location":"use-cases/#2-data-understanding","title":"2. Data Understanding","text":"<ul> <li>What data you need</li> <li>Where to find similar datasets</li> <li>Exploratory data analysis</li> </ul>"},{"location":"use-cases/#3-feature-engineering","title":"3. Feature Engineering","text":"<ul> <li>Domain-specific feature creation</li> <li>Handling data quality issues</li> <li>Feature selection strategies</li> </ul>"},{"location":"use-cases/#4-model-building","title":"4. Model Building","text":"<ul> <li>Algorithm selection rationale</li> <li>Training and validation approach</li> <li>Hyperparameter tuning</li> <li>Model evaluation and comparison</li> </ul>"},{"location":"use-cases/#5-deployment","title":"5. Deployment","text":"<ul> <li>Production considerations</li> <li>Monitoring and maintenance</li> <li>Common deployment patterns</li> </ul>"},{"location":"use-cases/#difficulty-levels","title":"Difficulty Levels","text":"Level Description Prerequisites  Beginner Good first use case Basic Python, pandas, scikit-learn  Intermediate More complex features/models Completed a beginner use case  Advanced Combines ML with optimization Solid ML foundation"},{"location":"use-cases/#want-to-contribute-a-use-case","title":"Want to Contribute a Use Case?","text":"<p>Are you solving interesting ML problems at work? Share your expertise with the community!</p> <p>We're looking for practitioners to contribute use cases in areas like:</p> <ul> <li>Fraud Detection - Financial services, e-commerce</li> <li>Customer Segmentation - Marketing, CRM</li> <li>Predictive Maintenance - Manufacturing, IoT</li> <li>Credit Scoring - Banking, lending</li> <li>Supply Chain Optimization - Logistics, manufacturing</li> </ul> <p> Learn How to Contribute</p>"},{"location":"use-cases/#getting-started","title":"Getting Started","text":"<p>New here? We recommend this path:</p> <ol> <li>Set up your environment - Get Python and Jupyter ready</li> <li>Start with Churn Modelling - Our most complete use case</li> <li>Explore datasets - Find data to practice with</li> <li>Apply to your own problem - The best way to learn!</li> </ol>"},{"location":"use-cases/churn-modelling/","title":"Churn Modelling","text":"<p>Predict which customers will leave before they do.</p> <p>Customer churn prediction is one of the most valuable applications of machine learning in business. A well-built churn model can save millions by enabling proactive retention strategies.</p>"},{"location":"use-cases/churn-modelling/#what-is-churn","title":"What is Churn?","text":"<p>Churn (also called attrition) occurs when a customer stops doing business with you. Depending on your business model:</p> Business Type Churn Definition Subscription (SaaS, Telecom) Customer cancels subscription Contractual (Banking, Insurance) Customer doesn't renew contract Non-contractual (Retail, E-commerce) Customer stops purchasing (requires definition) <p>The Non-Contractual Challenge</p> <p>In non-contractual businesses, there's no explicit \"cancellation\" event. You must define churn based on behavior\u2014e.g., \"no purchase in 90 days.\" This definition significantly impacts your model.</p>"},{"location":"use-cases/churn-modelling/#why-churn-prediction-matters","title":"Why Churn Prediction Matters","text":""},{"location":"use-cases/churn-modelling/#the-business-case","title":"The Business Case","text":"<p>Consider these typical numbers:</p> <ul> <li>Acquiring a new customer costs 5-25x more than retaining an existing one</li> <li>A 5% increase in retention can increase profits by 25-95%</li> <li>Churned customers rarely come back - win-back rates are typically &lt;10%</li> </ul>"},{"location":"use-cases/churn-modelling/#what-you-can-do-with-a-churn-model","title":"What You Can Do With a Churn Model","text":"<ol> <li>Proactive Retention - Reach out to at-risk customers before they leave</li> <li>Resource Allocation - Focus retention budget on customers most likely to churn</li> <li>Root Cause Analysis - Understand why customers leave</li> <li>Product Improvement - Identify features/issues driving churn</li> <li>Customer Lifetime Value - Better predict long-term value</li> </ol>"},{"location":"use-cases/churn-modelling/#the-business-problem","title":"The Business Problem","text":"<p>Scenario: Telecom Company</p> <p>You're a data scientist at a telecom company. The VP of Customer Success comes to you:</p> <p>\"We're losing customers faster than we can acquire them. Last quarter, 2,500 customers churned\u2014that's $3M in annual recurring revenue lost. We have a retention team, but they're calling customers randomly. Can you help us identify who's about to leave so we can intervene?\"</p>"},{"location":"use-cases/churn-modelling/#success-metrics","title":"Success Metrics","text":"<p>Before building any model, define success:</p> Metric Definition Target Precision Of predicted churners, how many actually churn? &gt;70% Recall Of actual churners, how many did we catch? &gt;60% Lift How much better than random targeting? &gt;3x Business Impact Revenue saved through retention &gt;$500K/quarter <p>Precision vs Recall Trade-off</p> <ul> <li>High Precision: Fewer false alarms, but miss some churners</li> <li>High Recall: Catch more churners, but waste resources on non-churners</li> </ul> <p>The right balance depends on your retention capacity and intervention cost.</p>"},{"location":"use-cases/churn-modelling/#approach-overview","title":"Approach Overview","text":"<pre><code>graph TD\n    A[Define Churn] --&gt; B[Collect Data]\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Train Model]\n    D --&gt; E[Evaluate Business Impact]\n    E --&gt; F[Deploy &amp; Monitor]\n    F --&gt; G[Iterate]\n    G --&gt; C</code></pre>"},{"location":"use-cases/churn-modelling/#what-well-cover","title":"What We'll Cover","text":"Section Description Time Data Understanding What data you need, EDA, data quality 45 min Feature Engineering Creating predictive features from raw data 60 min Model Building Training, tuning, and evaluation 90 min Deployment Taking your model to production 45 min"},{"location":"use-cases/churn-modelling/#quick-preview-what-youll-build","title":"Quick Preview: What You'll Build","text":"<p>By the end of this use case, you'll have:</p> <pre><code># Load your trained model\nimport joblib\nmodel = joblib.load('churn_model.pkl')\n\n# Score a customer\ncustomer_features = extract_features(customer_id='C12345')\nchurn_probability = model.predict_proba(customer_features)[0][1]\n\nprint(f\"Churn Risk: {churn_probability:.1%}\")\n# Output: Churn Risk: 73.2%\n\n# Get top risk factors\nexplain_prediction(model, customer_features)\n# Output: \n# - Contract: Month-to-month (+15% risk)\n# - Tenure: 3 months (+12% risk)\n# - Support tickets: 5 in last month (+10% risk)\n</code></pre>"},{"location":"use-cases/churn-modelling/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you're comfortable with:</p> <ul> <li> Python basics (functions, classes, data structures)</li> <li> pandas (DataFrames, groupby, merge)</li> <li> scikit-learn basics (fit, predict, train_test_split)</li> <li> Basic statistics (mean, median, correlation)</li> </ul> <p>Not there yet? Check our Getting Started guide.</p>"},{"location":"use-cases/churn-modelling/#datasets","title":"Datasets","text":"<p>We'll use the Telco Customer Churn dataset, a classic dataset for learning churn modeling:</p> Source Rows Features Link Kaggle 7,043 21 Download IBM Sample 7,043 21 Download <p>For practice with larger data, try:</p> Dataset Rows Description KKBox Churn 2.6M Music streaming service Santander 76K Banking"},{"location":"use-cases/churn-modelling/#ready-to-start","title":"Ready to Start?","text":""},{"location":"use-cases/churn-modelling/#data-understanding","title":"Data Understanding","text":"<p>Explore the data, understand distributions, and identify quality issues.</p> <p> Start Here</p>"},{"location":"use-cases/churn-modelling/#get-the-notebook","title":"Get the Notebook","text":"<p>Run the complete analysis in Google Colab or download to run locally.</p> <p></p>"},{"location":"use-cases/churn-modelling/data/","title":"Data Understanding","text":"<p>Before building any model, you need to deeply understand your data. This section covers loading the data, exploratory analysis, and identifying data quality issues.</p>"},{"location":"use-cases/churn-modelling/data/#loading-the-data","title":"Loading the Data","text":"<p>First, let's load the Telco Customer Churn dataset:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\nurl = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\ndf = pd.read_csv(url)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Columns: {df.columns.tolist()}\")\n</code></pre> <p>Output: <pre><code>Dataset shape: (7043, 21)\nColumns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', \n          'tenure', 'PhoneService', 'MultipleLines', 'InternetService', \n          'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n          'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n          'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n</code></pre></p>"},{"location":"use-cases/churn-modelling/data/#quick-data-overview","title":"Quick Data Overview","text":"<pre><code># Basic info\ndf.info()\n</code></pre> <pre><code>&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7043 non-null   object \n 2   SeniorCitizen     7043 non-null   int64  \n 3   Partner           7043 non-null   object \n 4   Dependents        7043 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7043 non-null   object \n ...\n 20  Churn             7043 non-null   object \n</code></pre> <pre><code># First few rows\ndf.head()\n</code></pre> customerID gender SeniorCitizen Partner tenure MonthlyCharges Churn 7590-VHVEG Female 0 Yes 1 29.85 No 5575-GNVDE Male 0 No 34 56.95 No 3668-QPYBK Male 0 No 2 53.85 Yes"},{"location":"use-cases/churn-modelling/data/#understanding-the-target-variable","title":"Understanding the Target Variable","text":"<pre><code># Churn distribution\nchurn_counts = df['Churn'].value_counts()\nchurn_pct = df['Churn'].value_counts(normalize=True) * 100\n\nprint(\"Churn Distribution:\")\nprint(f\"  No:  {churn_counts['No']:,} ({churn_pct['No']:.1f}%)\")\nprint(f\"  Yes: {churn_counts['Yes']:,} ({churn_pct['Yes']:.1f}%)\")\n</code></pre> <p>Output: <pre><code>Churn Distribution:\n  No:  5,174 (73.5%)\n  Yes: 1,869 (26.5%)\n</code></pre></p> <pre><code># Visualize\nfig, ax = plt.subplots(figsize=(8, 5))\ncolors = ['#2ecc71', '#e74c3c']\ndf['Churn'].value_counts().plot(kind='bar', color=colors, ax=ax)\nax.set_title('Churn Distribution', fontsize=14)\nax.set_xlabel('Churn')\nax.set_ylabel('Count')\nax.set_xticklabels(['No', 'Yes'], rotation=0)\n\n# Add percentages\nfor i, (count, pct) in enumerate(zip(churn_counts, churn_pct)):\n    ax.text(i, count + 100, f'{pct:.1f}%', ha='center', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Class Imbalance</p> <p>With ~27% churn rate, we have moderate class imbalance. This is actually realistic for many businesses. We'll address this during modeling.</p>"},{"location":"use-cases/churn-modelling/data/#feature-categories","title":"Feature Categories","text":"<p>Let's organize features by type:</p>"},{"location":"use-cases/churn-modelling/data/#demographic-features","title":"Demographic Features","text":"<pre><code>demographic_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents']\n\nfor col in demographic_cols:\n    print(f\"\\n{col}:\")\n    print(df[col].value_counts())\n</code></pre> Feature Values Notes gender Male (50.5%), Female (49.5%) Balanced SeniorCitizen No (83.8%), Yes (16.2%) Binary (0/1) Partner No (51.7%), Yes (48.3%) Balanced Dependents No (70.0%), Yes (30.0%) Slight imbalance"},{"location":"use-cases/churn-modelling/data/#service-features","title":"Service Features","text":"<pre><code>service_cols = ['PhoneService', 'MultipleLines', 'InternetService',\n                'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n                'TechSupport', 'StreamingTV', 'StreamingMovies']\n\n# Count services per customer\ndf['num_services'] = df[service_cols].apply(\n    lambda x: sum(x.isin(['Yes', 'Fiber optic', 'DSL'])), axis=1\n)\n</code></pre>"},{"location":"use-cases/churn-modelling/data/#account-features","title":"Account Features","text":"<pre><code>account_cols = ['Contract', 'PaperlessBilling', 'PaymentMethod', \n                'MonthlyCharges', 'TotalCharges', 'tenure']\n\n# Contract distribution\nprint(df['Contract'].value_counts())\n</code></pre> <pre><code>Month-to-month    3875\nTwo year          1695\nOne year          1473\n</code></pre> <p>Key Insight</p> <p>Month-to-month contracts are the most common\u2014and likely the highest churn risk. We'll verify this shortly.</p>"},{"location":"use-cases/churn-modelling/data/#data-quality-issues","title":"Data Quality Issues","text":""},{"location":"use-cases/churn-modelling/data/#missing-values","title":"Missing Values","text":"<pre><code># Check for missing values\nmissing = df.isnull().sum()\nmissing_pct = (missing / len(df)) * 100\n\nprint(\"Missing Values:\")\nprint(missing[missing &gt; 0])\n</code></pre> <pre><code>Missing Values:\nSeries([], dtype: int64)\n</code></pre> <p>Looks clean! But wait...</p> <pre><code># Check for hidden missing values\nprint(df['TotalCharges'].dtype)  # object - should be numeric!\n\n# Find non-numeric values\nnon_numeric = df[pd.to_numeric(df['TotalCharges'], errors='coerce').isna()]\nprint(f\"Non-numeric TotalCharges: {len(non_numeric)} rows\")\nprint(non_numeric[['customerID', 'tenure', 'TotalCharges']].head())\n</code></pre> <pre><code>Non-numeric TotalCharges: 11 rows\n     customerID  tenure TotalCharges\n488  4472-LVYGI       0             \n753  3115-CZMZD       0             \n...\n</code></pre> <p>Data Quality Issue Found</p> <p>11 customers have blank <code>TotalCharges</code>. These are all new customers (tenure=0). The blank likely means $0.</p> <p>Fix:</p> <pre><code># Convert TotalCharges to numeric, fill blanks with 0\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['TotalCharges'] = df['TotalCharges'].fillna(0)\n</code></pre>"},{"location":"use-cases/churn-modelling/data/#duplicate-check","title":"Duplicate Check","text":"<pre><code># Check for duplicate customer IDs\nduplicates = df['customerID'].duplicated().sum()\nprint(f\"Duplicate customer IDs: {duplicates}\")\n</code></pre> <pre><code>Duplicate customer IDs: 0\n</code></pre>"},{"location":"use-cases/churn-modelling/data/#exploratory-analysis","title":"Exploratory Analysis","text":""},{"location":"use-cases/churn-modelling/data/#churn-by-contract-type","title":"Churn by Contract Type","text":"<pre><code># Churn rate by contract\nchurn_by_contract = df.groupby('Contract')['Churn'].apply(\n    lambda x: (x == 'Yes').mean() * 100\n).sort_values(ascending=False)\n\nprint(\"Churn Rate by Contract:\")\nprint(churn_by_contract.round(1))\n</code></pre> <pre><code>Churn Rate by Contract:\nMonth-to-month    42.7%\nOne year          11.3%\nTwo year           2.8%\n</code></pre> <p>Key Finding</p> <p>Month-to-month customers churn at 15x the rate of two-year contract customers. Contract type will be a powerful predictor.</p>"},{"location":"use-cases/churn-modelling/data/#churn-by-tenure","title":"Churn by Tenure","text":"<pre><code># Tenure distribution by churn\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogram\nfor churn_val, color in [('No', '#2ecc71'), ('Yes', '#e74c3c')]:\n    subset = df[df['Churn'] == churn_val]\n    axes[0].hist(subset['tenure'], bins=30, alpha=0.6, \n                 label=f'Churn={churn_val}', color=color)\naxes[0].set_xlabel('Tenure (months)')\naxes[0].set_ylabel('Count')\naxes[0].set_title('Tenure Distribution by Churn')\naxes[0].legend()\n\n# Churn rate by tenure bucket\ndf['tenure_bucket'] = pd.cut(df['tenure'], \n                             bins=[0, 12, 24, 48, 72], \n                             labels=['0-12', '13-24', '25-48', '49-72'])\nchurn_by_tenure = df.groupby('tenure_bucket')['Churn'].apply(\n    lambda x: (x == 'Yes').mean() * 100\n)\naxes[1].bar(churn_by_tenure.index, churn_by_tenure.values, color='#3498db')\naxes[1].set_xlabel('Tenure (months)')\naxes[1].set_ylabel('Churn Rate (%)')\naxes[1].set_title('Churn Rate by Tenure')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Key Finding</p> <p>New customers (0-12 months) have the highest churn rate (~47%). Churn risk decreases significantly with tenure.</p>"},{"location":"use-cases/churn-modelling/data/#churn-by-monthly-charges","title":"Churn by Monthly Charges","text":"<pre><code># Monthly charges distribution\nfig, ax = plt.subplots(figsize=(10, 5))\n\ndf[df['Churn'] == 'No']['MonthlyCharges'].hist(\n    bins=30, alpha=0.6, label='No Churn', color='#2ecc71', ax=ax\n)\ndf[df['Churn'] == 'Yes']['MonthlyCharges'].hist(\n    bins=30, alpha=0.6, label='Churned', color='#e74c3c', ax=ax\n)\nax.set_xlabel('Monthly Charges ($)')\nax.set_ylabel('Count')\nax.set_title('Monthly Charges Distribution by Churn')\nax.legend()\nplt.show()\n\n# Average charges\nprint(f\"Avg Monthly Charges - Churned: ${df[df['Churn']=='Yes']['MonthlyCharges'].mean():.2f}\")\nprint(f\"Avg Monthly Charges - Retained: ${df[df['Churn']=='No']['MonthlyCharges'].mean():.2f}\")\n</code></pre> <pre><code>Avg Monthly Charges - Churned: $74.44\nAvg Monthly Charges - Retained: $61.27\n</code></pre> <p>Key Finding</p> <p>Churned customers pay $13 more per month on average. Higher-paying customers may have higher expectations or more alternatives.</p>"},{"location":"use-cases/churn-modelling/data/#correlation-analysis","title":"Correlation Analysis","text":"<pre><code># Encode categorical variables for correlation\ndf_encoded = df.copy()\ndf_encoded['Churn'] = (df_encoded['Churn'] == 'Yes').astype(int)\n\n# Select numeric columns\nnumeric_cols = df_encoded.select_dtypes(include=[np.number]).columns\ncorrelation = df_encoded[numeric_cols].corr()['Churn'].sort_values(ascending=False)\n\nprint(\"Correlation with Churn:\")\nprint(correlation.round(3))\n</code></pre> <pre><code>Correlation with Churn:\nChurn             1.000\nMonthlyCharges    0.193\nSeniorCitizen     0.150\nTotalCharges     -0.198\ntenure           -0.352\n</code></pre> <p>Interpretation</p> <ul> <li>Positive correlation: Higher monthly charges and being a senior citizen are associated with higher churn</li> <li>Negative correlation: Longer tenure and higher total charges (loyal customers) are associated with lower churn</li> </ul>"},{"location":"use-cases/churn-modelling/data/#key-takeaways","title":"Key Takeaways","text":"<p>From our EDA, we've identified the most important factors:</p> Factor Impact on Churn Strength Contract Type Month-to-month = high risk  Very Strong Tenure New customers = high risk  Very Strong Monthly Charges Higher charges = higher risk  Moderate Internet Service Fiber optic = higher risk  Moderate Senior Citizen Seniors = slightly higher risk  Weak"},{"location":"use-cases/churn-modelling/data/#data-preparation-summary","title":"Data Preparation Summary","text":"<pre><code># Final data preparation\ndef prepare_data(df):\n    \"\"\"Prepare raw data for modeling.\"\"\"\n    df = df.copy()\n\n    # Fix TotalCharges\n    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n    df['TotalCharges'] = df['TotalCharges'].fillna(0)\n\n    # Convert target to binary\n    df['Churn'] = (df['Churn'] == 'Yes').astype(int)\n\n    # Drop customerID (not predictive)\n    df = df.drop('customerID', axis=1)\n\n    return df\n\ndf_clean = prepare_data(df)\nprint(f\"Clean data shape: {df_clean.shape}\")\n</code></pre>"},{"location":"use-cases/churn-modelling/data/#next-steps","title":"Next Steps","text":"<p>Now that we understand our data, let's engineer features that capture customer behavior patterns.</p>"},{"location":"use-cases/churn-modelling/data/#previous","title":"Previous","text":"<p>Review the problem overview.</p> <p> Problem Overview</p>"},{"location":"use-cases/churn-modelling/data/#next","title":"Next","text":"<p>Create predictive features from raw data.</p> <p> Feature Engineering</p>"},{"location":"use-cases/churn-modelling/deployment/","title":"Deployment","text":"<p>A model in a notebook isn't useful. Let's explore how to deploy your churn model so it can actually impact business decisions.</p>"},{"location":"use-cases/churn-modelling/deployment/#deployment-options","title":"Deployment Options","text":"<p>Depending on your use case, choose the right deployment pattern:</p> Pattern Use Case Latency Complexity Batch Scoring Weekly/monthly risk lists Hours Low REST API Real-time decisions Milliseconds Medium Streaming Real-time event triggers Milliseconds High Embedded In-app predictions Milliseconds Medium <p>For churn prediction, batch scoring is usually sufficient\u2014you don't need real-time predictions to send a retention email.</p>"},{"location":"use-cases/churn-modelling/deployment/#option-1-batch-scoring","title":"Option 1: Batch Scoring","text":"<p>The simplest approach: score all customers periodically and store results.</p>"},{"location":"use-cases/churn-modelling/deployment/#scoring-script","title":"Scoring Script","text":"<pre><code># score_customers.py\nimport pandas as pd\nimport joblib\nfrom datetime import datetime\n\ndef load_model_artifacts():\n    \"\"\"Load trained model and preprocessing objects.\"\"\"\n    model = joblib.load('models/churn_model.pkl')\n    # Load any other artifacts (scalers, encoders, etc.)\n    return model\n\ndef extract_features(customer_data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Apply feature engineering pipeline.\"\"\"\n    # Import your feature engineering functions\n    from features import build_feature_matrix\n    return build_feature_matrix(customer_data)\n\ndef score_customers(customer_data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Score all customers and return risk scores.\"\"\"\n\n    # Load model\n    model = load_model_artifacts()\n\n    # Engineer features\n    features = extract_features(customer_data)\n\n    # Generate predictions\n    churn_probabilities = model.predict_proba(features)[:, 1]\n\n    # Create output dataframe\n    results = pd.DataFrame({\n        'customer_id': customer_data['customerID'],\n        'churn_probability': churn_probabilities,\n        'churn_risk_tier': pd.cut(\n            churn_probabilities, \n            bins=[0, 0.3, 0.6, 1.0],\n            labels=['Low', 'Medium', 'High']\n        ),\n        'scored_at': datetime.now()\n    })\n\n    return results.sort_values('churn_probability', ascending=False)\n\nif __name__ == '__main__':\n    # Load customer data from your data warehouse\n    customers = pd.read_sql(\n        \"SELECT * FROM customers WHERE is_active = 1\",\n        connection\n    )\n\n    # Score\n    scores = score_customers(customers)\n\n    # Save to database or file\n    scores.to_sql('churn_scores', connection, if_exists='replace')\n    scores.to_csv(f'scores/churn_scores_{datetime.now().date()}.csv')\n\n    print(f\"Scored {len(scores)} customers\")\n    print(f\"High risk: {(scores['churn_risk_tier'] == 'High').sum()}\")\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#scheduling-with-cron","title":"Scheduling with Cron","text":"<pre><code># Run every Monday at 6 AM\n0 6 * * 1 /usr/bin/python /path/to/score_customers.py &gt;&gt; /var/log/churn_scoring.log 2&gt;&amp;1\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#scheduling-with-airflow","title":"Scheduling with Airflow","text":"<pre><code># dags/churn_scoring_dag.py\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'churn_scoring',\n    default_args=default_args,\n    description='Weekly churn scoring pipeline',\n    schedule_interval='0 6 * * 1',  # Every Monday at 6 AM\n    catchup=False\n)\n\ndef run_scoring():\n    from score_customers import score_customers\n    # ... implementation\n\nscore_task = PythonOperator(\n    task_id='score_customers',\n    python_callable=run_scoring,\n    dag=dag\n)\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#option-2-rest-api","title":"Option 2: REST API","text":"<p>For real-time predictions (e.g., during a support call):</p>"},{"location":"use-cases/churn-modelling/deployment/#fastapi-implementation","title":"FastAPI Implementation","text":"<pre><code># api.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport joblib\nimport pandas as pd\n\napp = FastAPI(title=\"Churn Prediction API\")\n\n# Load model at startup\nmodel = None\n\n@app.on_event(\"startup\")\ndef load_model():\n    global model\n    model = joblib.load('models/churn_model.pkl')\n\nclass CustomerFeatures(BaseModel):\n    tenure: int\n    monthly_charges: float\n    total_charges: float\n    contract: str  # 'Month-to-month', 'One year', 'Two year'\n    payment_method: str\n    internet_service: str\n    # ... other features\n\nclass PredictionResponse(BaseModel):\n    customer_id: str\n    churn_probability: float\n    churn_risk: str\n    top_risk_factors: list\n\n@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict_churn(customer_id: str, features: CustomerFeatures):\n    \"\"\"Predict churn probability for a single customer.\"\"\"\n\n    if model is None:\n        raise HTTPException(status_code=500, detail=\"Model not loaded\")\n\n    # Convert to DataFrame\n    feature_dict = features.dict()\n    df = pd.DataFrame([feature_dict])\n\n    # Engineer features (simplified)\n    X = engineer_features(df)\n\n    # Predict\n    probability = model.predict_proba(X)[0][1]\n\n    # Determine risk tier\n    if probability &gt;= 0.6:\n        risk = \"High\"\n    elif probability &gt;= 0.3:\n        risk = \"Medium\"\n    else:\n        risk = \"Low\"\n\n    # Get top risk factors (simplified)\n    risk_factors = get_top_risk_factors(model, X, feature_dict)\n\n    return PredictionResponse(\n        customer_id=customer_id,\n        churn_probability=round(probability, 4),\n        churn_risk=risk,\n        top_risk_factors=risk_factors\n    )\n\n@app.get(\"/health\")\ndef health_check():\n    return {\"status\": \"healthy\", \"model_loaded\": model is not None}\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#running-the-api","title":"Running the API","text":"<pre><code># Install dependencies\npip install fastapi uvicorn\n\n# Run server\nuvicorn api:app --host 0.0.0.0 --port 8000\n\n# Test\ncurl -X POST \"http://localhost:8000/predict?customer_id=C12345\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"tenure\": 12, \"monthly_charges\": 75.0, ...}'\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY models/ models/\nCOPY api.py .\nCOPY features.py .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <pre><code># Build and run\ndocker build -t churn-api .\ndocker run -p 8000:8000 churn-api\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#model-monitoring","title":"Model Monitoring","text":"<p>Deployed models can degrade over time. Monitor these metrics:</p>"},{"location":"use-cases/churn-modelling/deployment/#1-prediction-distribution","title":"1. Prediction Distribution","text":"<pre><code># monitoring.py\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef check_prediction_drift(recent_scores: pd.DataFrame, \n                          historical_scores: pd.DataFrame,\n                          threshold: float = 0.1):\n    \"\"\"Check if prediction distribution has shifted.\"\"\"\n\n    recent_mean = recent_scores['churn_probability'].mean()\n    historical_mean = historical_scores['churn_probability'].mean()\n\n    drift = abs(recent_mean - historical_mean)\n\n    if drift &gt; threshold:\n        alert(f\"Prediction drift detected: {drift:.3f}\")\n        return True\n    return False\n\ndef check_feature_drift(recent_data: pd.DataFrame,\n                       historical_data: pd.DataFrame,\n                       features: list):\n    \"\"\"Check if input features have shifted.\"\"\"\n\n    drifted_features = []\n    for feature in features:\n        recent_mean = recent_data[feature].mean()\n        historical_mean = historical_data[feature].mean()\n\n        # Simple check - could use more sophisticated tests\n        if abs(recent_mean - historical_mean) / (historical_mean + 1e-10) &gt; 0.2:\n            drifted_features.append(feature)\n\n    if drifted_features:\n        alert(f\"Feature drift detected: {drifted_features}\")\n\n    return drifted_features\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#2-model-performance-when-labels-arrive","title":"2. Model Performance (When Labels Arrive)","text":"<pre><code>def evaluate_model_performance(predictions: pd.DataFrame,\n                              actuals: pd.DataFrame,\n                              threshold: float = 0.05):\n    \"\"\"Evaluate model against actual outcomes.\"\"\"\n\n    # Merge predictions with actual churn\n    merged = predictions.merge(actuals, on='customer_id')\n\n    # Calculate metrics\n    from sklearn.metrics import roc_auc_score, precision_score, recall_score\n\n    auc = roc_auc_score(merged['actual_churn'], merged['churn_probability'])\n\n    # Compare to baseline\n    baseline_auc = 0.85  # From training\n\n    if auc &lt; baseline_auc - threshold:\n        alert(f\"Model performance degraded: AUC {auc:.3f} vs baseline {baseline_auc:.3f}\")\n        return False\n\n    return True\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#3-dashboard-metrics","title":"3. Dashboard Metrics","text":"<p>Track these metrics on a dashboard:</p> Metric Description Alert Threshold Prediction Volume Scores generated per day &lt;90% of expected Mean Probability Average churn score &gt;20% change from baseline High Risk Count Customers in high risk tier &gt;50% change week-over-week Model Latency API response time (P95) &gt;500ms Error Rate Failed predictions &gt;1%"},{"location":"use-cases/churn-modelling/deployment/#retraining-strategy","title":"Retraining Strategy","text":""},{"location":"use-cases/churn-modelling/deployment/#when-to-retrain","title":"When to Retrain","text":"<ol> <li>Scheduled: Monthly or quarterly regardless of performance</li> <li>Performance-triggered: When metrics drop below threshold</li> <li>Data-triggered: When significant data drift is detected</li> </ol>"},{"location":"use-cases/churn-modelling/deployment/#retraining-pipeline","title":"Retraining Pipeline","text":"<pre><code># retrain_pipeline.py\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nimport mlflow\n\ndef retrain_model(training_data: pd.DataFrame):\n    \"\"\"Retrain churn model with new data.\"\"\"\n\n    # Start MLflow run\n    with mlflow.start_run(run_name=f\"retrain_{datetime.now().date()}\"):\n\n        # Prepare data\n        X = build_feature_matrix(training_data)\n        y = training_data['churned']\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, stratify=y\n        )\n\n        # Train model (same hyperparameters or re-tune)\n        model = XGBClassifier(**best_params)\n        model.fit(X_train, y_train)\n\n        # Evaluate\n        y_prob = model.predict_proba(X_test)[:, 1]\n        auc = roc_auc_score(y_test, y_prob)\n\n        # Log metrics\n        mlflow.log_metric(\"roc_auc\", auc)\n        mlflow.log_params(best_params)\n\n        # Compare to production model\n        prod_auc = get_production_model_auc()\n\n        if auc &gt; prod_auc:\n            # New model is better - promote to production\n            mlflow.sklearn.log_model(model, \"model\")\n            promote_to_production(model)\n            print(f\"New model promoted: AUC {auc:.3f} &gt; {prod_auc:.3f}\")\n        else:\n            print(f\"Keeping current model: {prod_auc:.3f} &gt;= {auc:.3f}\")\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#integration-with-business-processes","title":"Integration with Business Processes","text":"<p>The model is only valuable if it drives action:</p>"},{"location":"use-cases/churn-modelling/deployment/#1-crm-integration","title":"1. CRM Integration","text":"<pre><code># Push scores to Salesforce/HubSpot\ndef update_crm_scores(scores: pd.DataFrame):\n    \"\"\"Update customer churn scores in CRM.\"\"\"\n\n    for _, row in scores.iterrows():\n        crm_client.update_contact(\n            customer_id=row['customer_id'],\n            fields={\n                'churn_risk_score': row['churn_probability'],\n                'churn_risk_tier': row['churn_risk_tier'],\n                'scored_date': row['scored_at']\n            }\n        )\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#2-automated-campaigns","title":"2. Automated Campaigns","text":"<pre><code># Trigger retention campaigns\ndef trigger_retention_campaigns(scores: pd.DataFrame):\n    \"\"\"Trigger automated retention actions based on risk.\"\"\"\n\n    high_risk = scores[scores['churn_risk_tier'] == 'High']\n\n    for _, customer in high_risk.iterrows():\n        # Add to retention campaign\n        marketing_platform.add_to_campaign(\n            customer_id=customer['customer_id'],\n            campaign='high_risk_retention',\n            priority=customer['churn_probability']\n        )\n\n        # Create task for account manager\n        crm_client.create_task(\n            assigned_to=get_account_manager(customer['customer_id']),\n            subject=f\"High churn risk: {customer['customer_id']}\",\n            due_date=datetime.now() + timedelta(days=3)\n        )\n</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#3-reporting-dashboard","title":"3. Reporting Dashboard","text":"<p>Provide stakeholders with actionable insights:</p> <ul> <li>Weekly risk summary</li> <li>Trend analysis</li> <li>Campaign effectiveness</li> <li>Model performance metrics</li> </ul>"},{"location":"use-cases/churn-modelling/deployment/#production-checklist","title":"Production Checklist","text":"<p>Before deploying to production:</p> <ul> <li> Model validation: Tested on holdout data</li> <li> Code review: Scoring code reviewed by peer</li> <li> Integration tests: End-to-end pipeline tested</li> <li> Monitoring: Alerts configured for drift/errors</li> <li> Documentation: API docs, runbooks</li> <li> Rollback plan: Can revert to previous model</li> <li> Access control: Appropriate permissions set</li> <li> Logging: Predictions logged for audit</li> <li> Business sign-off: Stakeholders approved</li> </ul>"},{"location":"use-cases/churn-modelling/deployment/#summary","title":"Summary","text":"<p>You've completed the full churn modeling journey:</p> <pre><code>graph LR\n    A[Problem Definition] --&gt; B[Data Understanding]\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Model Building]\n    D --&gt; E[Deployment]\n    E --&gt; F[Monitoring]\n    F --&gt; G[Retraining]\n    G --&gt; D</code></pre>"},{"location":"use-cases/churn-modelling/deployment/#key-achievements","title":"Key Achievements","text":"<ul> <li> Defined the business problem and success metrics</li> <li> Explored and cleaned the data</li> <li> Engineered 38 predictive features</li> <li> Trained and compared multiple models</li> <li> Achieved 0.856 ROC-AUC with XGBoost</li> <li> Optimized threshold for business constraints</li> <li> Learned deployment patterns and monitoring</li> </ul>"},{"location":"use-cases/churn-modelling/deployment/#next-steps","title":"Next Steps","text":""},{"location":"use-cases/churn-modelling/deployment/#get-the-complete-code","title":"Get the Complete Code","text":"<p>Download the Jupyter notebook with all code from this use case.</p> <p></p>"},{"location":"use-cases/churn-modelling/deployment/#try-another-use-case","title":"Try Another Use Case","text":"<p>Apply what you've learned to a different problem.</p> <p> Browse Use Cases</p>"},{"location":"use-cases/churn-modelling/deployment/#want-to-contribute","title":"Want to Contribute?","text":"<p>Have you built a churn model at your company? Share your learnings!</p> <p> Become a Contributor</p>"},{"location":"use-cases/churn-modelling/features/","title":"Feature Engineering","text":"<p>Feature engineering is where domain knowledge meets data science. Good features can make a simple model outperform a complex one with poor features.</p>"},{"location":"use-cases/churn-modelling/features/#why-feature-engineering-matters","title":"Why Feature Engineering Matters","text":"<p>\"Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.\" \u2014 Andrew Ng</p> <p>In churn modeling, raw data rarely captures the full picture. A customer's behavior patterns and changes over time are often more predictive than static attributes.</p>"},{"location":"use-cases/churn-modelling/features/#feature-engineering-strategy","title":"Feature Engineering Strategy","text":"<p>We'll create features in four categories:</p> <pre><code>graph TD\n    A[Raw Data] --&gt; B[Demographic Features]\n    A --&gt; C[Service Features]\n    A --&gt; D[Behavioral Features]\n    A --&gt; E[Derived Features]\n    B --&gt; F[Feature Matrix]\n    C --&gt; F\n    D --&gt; F\n    E --&gt; F</code></pre>"},{"location":"use-cases/churn-modelling/features/#1-demographic-features","title":"1. Demographic Features","text":"<p>These capture who the customer is:</p> <pre><code>def create_demographic_features(df):\n    \"\"\"Create features from demographic data.\"\"\"\n    features = pd.DataFrame()\n\n    # Binary encoding\n    features['is_senior'] = df['SeniorCitizen']\n    features['has_partner'] = (df['Partner'] == 'Yes').astype(int)\n    features['has_dependents'] = (df['Dependents'] == 'Yes').astype(int)\n\n    # Combined features\n    features['family_size'] = features['has_partner'] + features['has_dependents']\n    features['is_single'] = ((features['has_partner'] == 0) &amp; \n                             (features['has_dependents'] == 0)).astype(int)\n\n    # Gender (for completeness, though often not predictive)\n    features['is_male'] = (df['gender'] == 'Male').astype(int)\n\n    return features\n\ndemo_features = create_demographic_features(df)\nprint(demo_features.head())\n</code></pre> is_senior has_partner has_dependents family_size is_single 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 <p>Feature Insight</p> <p>Single customers without dependents may have fewer switching costs, making them higher churn risks.</p>"},{"location":"use-cases/churn-modelling/features/#2-service-features","title":"2. Service Features","text":"<p>Capture what services the customer uses:</p> <pre><code>def create_service_features(df):\n    \"\"\"Create features from service subscription data.\"\"\"\n    features = pd.DataFrame()\n\n    # Phone services\n    features['has_phone'] = (df['PhoneService'] == 'Yes').astype(int)\n    features['has_multiple_lines'] = (df['MultipleLines'] == 'Yes').astype(int)\n\n    # Internet service type\n    features['has_internet'] = (df['InternetService'] != 'No').astype(int)\n    features['has_fiber'] = (df['InternetService'] == 'Fiber optic').astype(int)\n    features['has_dsl'] = (df['InternetService'] == 'DSL').astype(int)\n\n    # Add-on services (only available with internet)\n    addon_services = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n                      'TechSupport', 'StreamingTV', 'StreamingMovies']\n\n    for service in addon_services:\n        features[f'has_{service.lower()}'] = (df[service] == 'Yes').astype(int)\n\n    # Count of add-on services\n    features['num_addons'] = features[[f'has_{s.lower()}' for s in addon_services]].sum(axis=1)\n\n    # Security/support bundle\n    features['has_security_bundle'] = (\n        (features['has_onlinesecurity'] == 1) &amp; \n        (features['has_techsupport'] == 1)\n    ).astype(int)\n\n    # Streaming bundle\n    features['has_streaming_bundle'] = (\n        (features['has_streamingtv'] == 1) &amp; \n        (features['has_streamingmovies'] == 1)\n    ).astype(int)\n\n    # Total services\n    features['total_services'] = (\n        features['has_phone'] + \n        features['has_internet'] + \n        features['num_addons']\n    )\n\n    return features\n\nservice_features = create_service_features(df)\nprint(f\"Service features: {service_features.columns.tolist()}\")\n</code></pre> <p>Key Insight</p> <p>Customers with more services typically have lower churn\u2014they're more invested in the ecosystem. We'll verify this:</p> <pre><code># Churn rate by number of services\nchurn_by_services = df.copy()\nchurn_by_services['total_services'] = service_features['total_services']\nchurn_rate = churn_by_services.groupby('total_services')['Churn'].apply(\n    lambda x: (x == 'Yes').mean() * 100\n)\nprint(\"Churn Rate by Total Services:\")\nprint(churn_rate.round(1))\n</code></pre> <pre><code>Churn Rate by Total Services:\n1     53.2%\n2     35.1%\n3     28.4%\n4     22.1%\n5     18.3%\n6+    14.2%\n</code></pre>"},{"location":"use-cases/churn-modelling/features/#3-account-contract-features","title":"3. Account &amp; Contract Features","text":"<p>Capture the customer's business relationship:</p> <pre><code>def create_account_features(df):\n    \"\"\"Create features from account and billing data.\"\"\"\n    features = pd.DataFrame()\n\n    # Contract type (one-hot encoding)\n    features['contract_monthly'] = (df['Contract'] == 'Month-to-month').astype(int)\n    features['contract_one_year'] = (df['Contract'] == 'One year').astype(int)\n    features['contract_two_year'] = (df['Contract'] == 'Two year').astype(int)\n\n    # Billing\n    features['paperless_billing'] = (df['PaperlessBilling'] == 'Yes').astype(int)\n\n    # Payment method\n    features['payment_electronic'] = (df['PaymentMethod'] == 'Electronic check').astype(int)\n    features['payment_auto'] = df['PaymentMethod'].str.contains('automatic').astype(int)\n\n    # Tenure\n    features['tenure'] = df['tenure']\n    features['tenure_months'] = df['tenure']\n\n    # Tenure buckets\n    features['is_new_customer'] = (df['tenure'] &lt;= 6).astype(int)\n    features['is_established'] = (df['tenure'] &gt; 24).astype(int)\n\n    # Charges\n    features['monthly_charges'] = df['MonthlyCharges']\n    features['total_charges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)\n\n    return features\n\naccount_features = create_account_features(df)\n</code></pre> <p>Electronic Check = High Risk</p> <p>Customers paying by electronic check have significantly higher churn. This payment method may indicate:</p> <ul> <li>Less commitment (not setting up auto-pay)</li> <li>Potential payment issues</li> <li>Lower engagement with the service</li> </ul> <pre><code># Verify payment method impact\nchurn_by_payment = df.groupby('PaymentMethod')['Churn'].apply(\n    lambda x: (x == 'Yes').mean() * 100\n).sort_values(ascending=False)\nprint(churn_by_payment.round(1))\n</code></pre> <pre><code>Electronic check             45.3%\nMailed check                 19.1%\nBank transfer (automatic)    16.7%\nCredit card (automatic)      15.2%\n</code></pre>"},{"location":"use-cases/churn-modelling/features/#4-derived-features","title":"4. Derived Features","text":"<p>These capture relationships and patterns:</p> <pre><code>def create_derived_features(df, service_features, account_features):\n    \"\"\"Create derived features from combinations of other features.\"\"\"\n    features = pd.DataFrame()\n\n    # Average monthly spend (for customers with tenure &gt; 0)\n    features['avg_monthly_spend'] = np.where(\n        df['tenure'] &gt; 0,\n        pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0) / df['tenure'],\n        df['MonthlyCharges']\n    )\n\n    # Charge change (current vs average)\n    features['charge_vs_avg'] = df['MonthlyCharges'] - features['avg_monthly_spend']\n\n    # Revenue per service\n    features['revenue_per_service'] = np.where(\n        service_features['total_services'] &gt; 0,\n        df['MonthlyCharges'] / service_features['total_services'],\n        df['MonthlyCharges']\n    )\n\n    # Contract value remaining (rough estimate)\n    contract_months = df['Contract'].map({\n        'Month-to-month': 1,\n        'One year': 12,\n        'Two year': 24\n    })\n    features['months_in_contract'] = df['tenure'] % contract_months\n\n    # Tenure to charges ratio\n    features['tenure_charges_ratio'] = np.where(\n        df['MonthlyCharges'] &gt; 0,\n        df['tenure'] / df['MonthlyCharges'],\n        0\n    )\n\n    # High value customer (top quartile of monthly charges)\n    charge_threshold = df['MonthlyCharges'].quantile(0.75)\n    features['is_high_value'] = (df['MonthlyCharges'] &gt;= charge_threshold).astype(int)\n\n    # Risk score (simple rule-based)\n    features['rule_based_risk'] = (\n        account_features['contract_monthly'] * 3 +\n        account_features['is_new_customer'] * 2 +\n        account_features['payment_electronic'] * 2 +\n        (1 - service_features['has_security_bundle']) * 1\n    )\n\n    return features\n\nderived_features = create_derived_features(df, service_features, account_features)\n</code></pre>"},{"location":"use-cases/churn-modelling/features/#combining-all-features","title":"Combining All Features","text":"<pre><code>def build_feature_matrix(df):\n    \"\"\"Build complete feature matrix from raw data.\"\"\"\n\n    # Create all feature groups\n    demo = create_demographic_features(df)\n    service = create_service_features(df)\n    account = create_account_features(df)\n    derived = create_derived_features(df, service, account)\n\n    # Combine\n    features = pd.concat([demo, service, account, derived], axis=1)\n\n    # Remove any duplicate columns\n    features = features.loc[:, ~features.columns.duplicated()]\n\n    return features\n\n# Build feature matrix\nX = build_feature_matrix(df)\ny = (df['Churn'] == 'Yes').astype(int)\n\nprint(f\"Feature matrix shape: {X.shape}\")\nprint(f\"Features: {X.columns.tolist()}\")\n</code></pre> <pre><code>Feature matrix shape: (7043, 38)\nFeatures: ['is_senior', 'has_partner', 'has_dependents', 'family_size', \n           'is_single', 'is_male', 'has_phone', 'has_multiple_lines', ...]\n</code></pre>"},{"location":"use-cases/churn-modelling/features/#feature-selection","title":"Feature Selection","text":"<p>Not all features are equally useful. Let's identify the most predictive ones:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Train a quick Random Forest for feature importance\nrf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nrf.fit(X_train, y_train)\n\n# Get feature importances\nimportance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 15 Features:\")\nprint(importance.head(15))\n</code></pre> <pre><code>Top 15 Features:\n                feature  importance\n0                tenure       0.168\n1       monthly_charges       0.142\n2         total_charges       0.128\n3      contract_monthly       0.089\n4     avg_monthly_spend       0.067\n5   tenure_charges_ratio     0.054\n6       rule_based_risk       0.048\n7    payment_electronic       0.041\n8        total_services       0.038\n9       is_new_customer       0.035\n10           num_addons       0.032\n11            has_fiber       0.028\n12     has_techsupport        0.024\n13  has_onlinesecurity       0.023\n14       is_established       0.021\n</code></pre> <pre><code># Visualize\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 8))\ntop_features = importance.head(15)\nax.barh(top_features['feature'], top_features['importance'])\nax.set_xlabel('Importance')\nax.set_title('Top 15 Features by Importance')\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"use-cases/churn-modelling/features/#handling-categorical-variables","title":"Handling Categorical Variables","text":"<p>For some models (like Logistic Regression), we need to properly encode categorical variables:</p> <pre><code>from sklearn.preprocessing import StandardScaler, LabelEncoder\n\ndef prepare_features_for_modeling(X, scale=True):\n    \"\"\"Prepare features for model training.\"\"\"\n    X_prepared = X.copy()\n\n    # All our features are already numeric, but let's ensure\n    for col in X_prepared.columns:\n        if X_prepared[col].dtype == 'object':\n            le = LabelEncoder()\n            X_prepared[col] = le.fit_transform(X_prepared[col].astype(str))\n\n    # Scale features (important for some algorithms)\n    if scale:\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X_prepared)\n        X_prepared = pd.DataFrame(X_scaled, columns=X_prepared.columns)\n\n    return X_prepared\n\nX_scaled = prepare_features_for_modeling(X, scale=True)\nprint(X_scaled.describe().round(2))\n</code></pre>"},{"location":"use-cases/churn-modelling/features/#feature-engineering-best-practices","title":"Feature Engineering Best Practices","text":"<p>Best Practices</p> <ol> <li>Start with domain knowledge - What would YOU look at to predict churn?</li> <li>Create interaction features - Combinations often outperform individual features</li> <li>Capture changes over time - Trends matter more than snapshots</li> <li>Handle edge cases - Division by zero, missing values, etc.</li> <li>Validate with EDA - Check that features behave as expected</li> <li>Avoid data leakage - Don't use future information to predict past events</li> </ol> <p>Common Mistakes</p> <ul> <li>Using customer ID as a feature (memorization, not learning)</li> <li>Including the target variable in features</li> <li>Creating features from data you won't have at prediction time</li> <li>Over-engineering features without validation</li> </ul>"},{"location":"use-cases/churn-modelling/features/#save-feature-engineering-pipeline","title":"Save Feature Engineering Pipeline","text":"<pre><code>import joblib\n\n# Save the feature engineering functions for production use\nfeature_pipeline = {\n    'create_demographic_features': create_demographic_features,\n    'create_service_features': create_service_features,\n    'create_account_features': create_account_features,\n    'create_derived_features': create_derived_features,\n    'build_feature_matrix': build_feature_matrix,\n}\n\n# In production, you'd save this as a proper pipeline\n# joblib.dump(feature_pipeline, 'feature_pipeline.pkl')\n</code></pre>"},{"location":"use-cases/churn-modelling/features/#next-steps","title":"Next Steps","text":"<p>Our features are ready! Now let's train and evaluate models.</p>"},{"location":"use-cases/churn-modelling/features/#previous","title":"Previous","text":"<p>Review the data analysis.</p> <p> Data Understanding</p>"},{"location":"use-cases/churn-modelling/features/#next","title":"Next","text":"<p>Train and evaluate churn prediction models.</p> <p> Model Building</p>"},{"location":"use-cases/churn-modelling/modelling/","title":"Model Building","text":"<p>Now that we have our features, let's train and evaluate models. We'll compare multiple algorithms and select the best one for production.</p>"},{"location":"use-cases/churn-modelling/modelling/#setup","title":"Setup","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n    precision_recall_curve, roc_curve\n)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load and prepare data (from previous sections)\n# X = build_feature_matrix(df)\n# y = (df['Churn'] == 'Yes').astype(int)\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#traintest-split","title":"Train/Test Split","text":"<pre><code># Split data with stratification (maintain class balance)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42, \n    stratify=y  # Important for imbalanced data\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"Churn rate (train): {y_train.mean():.1%}\")\nprint(f\"Churn rate (test): {y_test.mean():.1%}\")\n</code></pre> <pre><code>Training set: 5634 samples\nTest set: 1409 samples\nChurn rate (train): 26.5%\nChurn rate (test): 26.5%\n</code></pre> <p>Why Stratify?</p> <p>Stratification ensures both train and test sets have the same class distribution. Without it, you might accidentally get all churners in one set.</p>"},{"location":"use-cases/churn-modelling/modelling/#baseline-model","title":"Baseline Model","text":"<p>Always start with a simple baseline:</p> <pre><code># Baseline: Always predict the majority class\nbaseline_accuracy = 1 - y_test.mean()\nprint(f\"Baseline Accuracy (predict all 'No Churn'): {baseline_accuracy:.1%}\")\n\n# A useful model must beat this!\n</code></pre> <pre><code>Baseline Accuracy (predict all 'No Churn'): 73.5%\n</code></pre> <p>Accuracy is Misleading</p> <p>With 73.5% non-churners, predicting \"No Churn\" for everyone gives 73.5% accuracy but catches zero actual churners. We need better metrics.</p>"},{"location":"use-cases/churn-modelling/modelling/#model-1-logistic-regression","title":"Model 1: Logistic Regression","text":"<p>A great starting point\u2014interpretable and fast:</p> <pre><code># Scale features (important for Logistic Regression)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train model\nlr = LogisticRegression(\n    max_iter=1000,\n    class_weight='balanced',  # Handle class imbalance\n    random_state=42\n)\nlr.fit(X_train_scaled, y_train)\n\n# Predictions\ny_pred_lr = lr.predict(X_test_scaled)\ny_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]\n\n# Evaluate\nprint(\"Logistic Regression Results:\")\nprint(classification_report(y_test, y_pred_lr))\nprint(f\"ROC-AUC: {roc_auc_score(y_test, y_prob_lr):.3f}\")\n</code></pre> <pre><code>Logistic Regression Results:\n              precision    recall  f1-score   support\n\n           0       0.88      0.76      0.82      1036\n           1       0.55      0.74      0.63       373\n\n    accuracy                           0.76      1409\n   macro avg       0.72      0.75      0.72      1409\nweighted avg       0.79      0.76      0.77      1409\n\nROC-AUC: 0.838\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#feature-coefficients","title":"Feature Coefficients","text":"<pre><code># Get feature importance from coefficients\ncoef_df = pd.DataFrame({\n    'feature': X.columns,\n    'coefficient': lr.coef_[0]\n}).sort_values('coefficient', key=abs, ascending=False)\n\nprint(\"Top 10 Features (by absolute coefficient):\")\nprint(coef_df.head(10))\n</code></pre> <pre><code>Top 10 Features:\n                 feature  coefficient\n0       contract_monthly        1.234\n1                 tenure       -0.987\n2     payment_electronic        0.654\n3         total_services       -0.543\n4        is_new_customer        0.498\n...\n</code></pre> <p>Interpretability</p> <p>Logistic Regression tells us exactly how each feature affects churn probability. Positive coefficients increase churn risk.</p>"},{"location":"use-cases/churn-modelling/modelling/#model-2-random-forest","title":"Model 2: Random Forest","text":"<p>An ensemble method that often performs better:</p> <pre><code># Train Random Forest\nrf = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=10,\n    min_samples_split=10,\n    class_weight='balanced',\n    random_state=42,\n    n_jobs=-1\n)\nrf.fit(X_train, y_train)\n\n# Predictions\ny_pred_rf = rf.predict(X_test)\ny_prob_rf = rf.predict_proba(X_test)[:, 1]\n\n# Evaluate\nprint(\"Random Forest Results:\")\nprint(classification_report(y_test, y_pred_rf))\nprint(f\"ROC-AUC: {roc_auc_score(y_test, y_prob_rf):.3f}\")\n</code></pre> <pre><code>Random Forest Results:\n              precision    recall  f1-score   support\n\n           0       0.87      0.82      0.85      1036\n           1       0.59      0.68      0.63       373\n\n    accuracy                           0.79      1409\n   macro avg       0.73      0.75      0.74      1409\nweighted avg       0.80      0.79      0.79      1409\n\nROC-AUC: 0.847\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#model-3-gradient-boosting-xgboost","title":"Model 3: Gradient Boosting (XGBoost)","text":"<p>State-of-the-art for tabular data:</p> <pre><code>try:\n    from xgboost import XGBClassifier\n\n    # Calculate scale_pos_weight for imbalanced data\n    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n\n    xgb = XGBClassifier(\n        n_estimators=200,\n        max_depth=5,\n        learning_rate=0.1,\n        scale_pos_weight=scale_pos_weight,\n        random_state=42,\n        use_label_encoder=False,\n        eval_metric='logloss'\n    )\n    xgb.fit(X_train, y_train)\n\n    y_pred_xgb = xgb.predict(X_test)\n    y_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n\n    print(\"XGBoost Results:\")\n    print(classification_report(y_test, y_pred_xgb))\n    print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob_xgb):.3f}\")\n\nexcept ImportError:\n    print(\"XGBoost not installed. Run: pip install xgboost\")\n</code></pre> <pre><code>XGBoost Results:\n              precision    recall  f1-score   support\n\n           0       0.88      0.84      0.86      1036\n           1       0.62      0.70      0.66       373\n\n    accuracy                           0.80      1409\n   macro avg       0.75      0.77      0.76      1409\nweighted avg       0.81      0.80      0.81      1409\n\nROC-AUC: 0.856\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#model-comparison","title":"Model Comparison","text":"<pre><code># Compare all models\nmodels = {\n    'Logistic Regression': (y_pred_lr, y_prob_lr),\n    'Random Forest': (y_pred_rf, y_prob_rf),\n    'XGBoost': (y_pred_xgb, y_prob_xgb)\n}\n\nresults = []\nfor name, (y_pred, y_prob) in models.items():\n    results.append({\n        'Model': name,\n        'Accuracy': accuracy_score(y_test, y_pred),\n        'Precision': precision_score(y_test, y_pred),\n        'Recall': recall_score(y_test, y_pred),\n        'F1': f1_score(y_test, y_pred),\n        'ROC-AUC': roc_auc_score(y_test, y_prob)\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df.round(3))\n</code></pre> Model Accuracy Precision Recall F1 ROC-AUC Logistic Regression 0.758 0.553 0.740 0.633 0.838 Random Forest 0.785 0.589 0.684 0.633 0.847 XGBoost 0.804 0.618 0.700 0.657 0.856"},{"location":"use-cases/churn-modelling/modelling/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Let's tune the best-performing model (XGBoost):</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'min_child_weight': [1, 3, 5]\n}\n\nxgb_tuned = XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    random_state=42,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    xgb_tuned, \n    param_grid, \n    cv=5, \n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1\n)\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV ROC-AUC: {grid_search.best_score_:.3f}\")\n</code></pre> <pre><code>Best parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200}\nBest CV ROC-AUC: 0.851\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#threshold-optimization","title":"Threshold Optimization","text":"<p>The default threshold (0.5) might not be optimal for business:</p> <pre><code># Get precision-recall for different thresholds\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob_xgb)\n\n# Find threshold that maximizes F1\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\nbest_threshold_idx = np.argmax(f1_scores)\nbest_threshold = thresholds[best_threshold_idx]\n\nprint(f\"Optimal threshold: {best_threshold:.3f}\")\nprint(f\"At this threshold:\")\nprint(f\"  Precision: {precision[best_threshold_idx]:.3f}\")\nprint(f\"  Recall: {recall[best_threshold_idx]:.3f}\")\nprint(f\"  F1: {f1_scores[best_threshold_idx]:.3f}\")\n\n# Plot precision-recall curve\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(recall, precision, 'b-', linewidth=2)\nax.scatter([recall[best_threshold_idx]], [precision[best_threshold_idx]], \n           color='red', s=100, zorder=5, label=f'Optimal (threshold={best_threshold:.2f})')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_title('Precision-Recall Curve')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#business-oriented-threshold-selection","title":"Business-Oriented Threshold Selection","text":"<pre><code># Consider business constraints\n# Scenario: Retention team can contact 500 customers/month\n\n# Sort customers by churn probability\ntest_df = pd.DataFrame({\n    'actual': y_test.values,\n    'probability': y_prob_xgb\n}).sort_values('probability', ascending=False)\n\n# Top 500 customers\ntop_500 = test_df.head(500)\nchurners_caught = top_500['actual'].sum()\ntotal_churners = y_test.sum()\n\nprint(f\"If we contact top 500 highest-risk customers:\")\nprint(f\"  Churners caught: {churners_caught} out of {total_churners}\")\nprint(f\"  Catch rate: {churners_caught/total_churners:.1%}\")\nprint(f\"  Precision (of 500 contacted): {churners_caught/500:.1%}\")\n</code></pre> <pre><code>If we contact top 500 highest-risk customers:\n  Churners caught: 289 out of 373\n  Catch rate: 77.5%\n  Precision (of 500 contacted): 57.8%\n</code></pre> <p>Business Impact</p> <p>By scoring customers, we can catch 77.5% of churners by contacting only the top 35% of the customer base. That's a 2.2x lift over random targeting!</p>"},{"location":"use-cases/churn-modelling/modelling/#confusion-matrix-analysis","title":"Confusion Matrix Analysis","text":"<pre><code># Apply optimized threshold\ny_pred_optimized = (y_prob_xgb &gt;= best_threshold).astype(int)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred_optimized)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n            xticklabels=['Predicted: Stay', 'Predicted: Churn'],\n            yticklabels=['Actual: Stay', 'Actual: Churn'])\nax.set_title('Confusion Matrix')\nplt.show()\n\n# Interpret\ntn, fp, fn, tp = cm.ravel()\nprint(f\"True Negatives (correctly predicted stay): {tn}\")\nprint(f\"False Positives (incorrectly predicted churn): {fp}\")\nprint(f\"False Negatives (missed churners): {fn}\")\nprint(f\"True Positives (correctly predicted churn): {tp}\")\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#model-interpretability","title":"Model Interpretability","text":"<p>Understanding why the model makes predictions:</p> <pre><code># Feature importance from XGBoost\nimportance_xgb = pd.DataFrame({\n    'feature': X.columns,\n    'importance': grid_search.best_estimator_.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 8))\ntop_15 = importance_xgb.head(15)\nax.barh(top_15['feature'], top_15['importance'])\nax.set_xlabel('Importance')\nax.set_title('Top 15 Features - XGBoost')\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#shap-values-advanced","title":"SHAP Values (Advanced)","text":"<pre><code>try:\n    import shap\n\n    # Calculate SHAP values\n    explainer = shap.TreeExplainer(grid_search.best_estimator_)\n    shap_values = explainer.shap_values(X_test)\n\n    # Summary plot\n    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n\nexcept ImportError:\n    print(\"SHAP not installed. Run: pip install shap\")\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#final-model","title":"Final Model","text":"<pre><code># Train final model on full training data with best parameters\nfinal_model = XGBClassifier(\n    **grid_search.best_params_,\n    scale_pos_weight=scale_pos_weight,\n    random_state=42,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\nfinal_model.fit(X_train, y_train)\n\n# Save model\nimport joblib\njoblib.dump(final_model, 'churn_model.pkl')\njoblib.dump(scaler, 'feature_scaler.pkl')\n\nprint(\"Model saved successfully!\")\n</code></pre>"},{"location":"use-cases/churn-modelling/modelling/#key-takeaways","title":"Key Takeaways","text":"Aspect Finding Best Model XGBoost with ROC-AUC of 0.856 Top Features Contract type, tenure, payment method Optimal Threshold ~0.35 for balanced precision/recall Business Lift 2.2x better than random targeting"},{"location":"use-cases/churn-modelling/modelling/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to deploy this model in production.</p>"},{"location":"use-cases/churn-modelling/modelling/#previous","title":"Previous","text":"<p>Review feature engineering.</p> <p> Feature Engineering</p>"},{"location":"use-cases/churn-modelling/modelling/#next","title":"Next","text":"<p>Deploy your model to production.</p> <p> Deployment</p>"}]}